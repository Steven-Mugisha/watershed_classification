{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca import PCA_Analysis\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load .env file and pca class\n",
    "load_dotenv()\n",
    "directory = os.getenv(\"path\")\n",
    "output_dir = os.getenv(\"output_dir\")\n",
    "pca = PCA_Analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structural attributes (Time-invariant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_pca() -> dict:\n",
    "    struct_path = os.path.join(directory, \"data\",\"raw_datasets\", \"structural_attributes\", \"140_TI_variables\")\n",
    "    stations_list = pd.read_csv(os.path.join(directory,  \"data\",\"raw_datasets\", \"structural_attributes\", \"stations_list.csv\")).stations.tolist()\n",
    "\n",
    "\n",
    "    struct_files = os.listdir(struct_path)\n",
    "    struct_files = [file for file in struct_files if file.endswith(\".csv\")]\n",
    "\n",
    "    all_top_load_dfs, all_top_attr = [], {}\n",
    "\n",
    "    for file in struct_files:\n",
    "        attr_type = file.split(\"_\")[1].split(\".\")[0]\n",
    "        input_struct_data = pd.read_csv(os.path.join(struct_path, file))\n",
    "\n",
    "        input_struct_data = input_struct_data[input_struct_data[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        struct_data = pd.concat([input_struct_data[\"station_id\"], input_struct_data.iloc[:, 4:]], axis = 1)\n",
    "        struct_data = struct_data.set_index(\"station_id\")\n",
    "\n",
    "        # data = pca.pca_analysis(struct_data)\n",
    "        loadings = pca.loadings(struct_data)\n",
    "\n",
    "        explained_var = pca.explained_variance(struct_data)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_attr = pca.top_attributes(loadings, 5)\n",
    "        all_top_attr[attr_type] = top_attr\n",
    "        new_keys = {'PC1': f'{attr_type}_PC1_{pc1_val}', 'PC2': f'{attr_type}_PC2_{pc2_val}'}\n",
    "        renamed_top_attr = {new_keys[key]: value for key, value in top_attr.items()}\n",
    "        top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "        all_top_load_dfs.append(top_load_df)\n",
    "    # return  pd.concat(all_top_load_dfs, axis = 1)\n",
    "    # out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"TI_top_attributes.csv\"), index=False)\n",
    "    return all_top_attr\n",
    "\n",
    "structural_pca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop inventories (Time-variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_pca() -> dict:\n",
    "    crops_path = os.path.join(directory, \"data\", \"raw_datasets\", \"structural_attributes\", \"crop_inventories\")\n",
    "\n",
    "    crop_inventories_files = glob.glob(f\"{crops_path}/*.csv\")\n",
    "\n",
    "    all_crop_top_load_dfs, all_top_attr  = [], {}\n",
    "\n",
    "    for path in crop_inventories_files:\n",
    "        crop_yr_df = pd.read_csv(path)\n",
    "        crop_yr_df = crop_yr_df.set_index(crop_yr_df.columns[0])\n",
    "\n",
    "        # crop pca analysis\n",
    "        # pca_df = pca.pca_analysis(crop_yr_df)\n",
    "        loadings = pca.loadings(crop_yr_df)\n",
    "\n",
    "        explained_var = pca.explained_variance(crop_yr_df)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_attr = pca.top_attributes(loadings, 5)\n",
    "        all_top_attr[path.split(\"/\")[-1].split(\".\")[0]] = top_attr\n",
    "        new_keys = {'PC1': f'{path.split(\"/\")[-1].split(\".\")[0]}_PC1_{pc1_val}', 'PC2': f'{path.split(\"/\")[-1].split(\".\")[0]}_PC2_{pc2_val}'}\n",
    "        renamed_top_attr = {new_keys[key]: value for key, value in top_attr.items()}\n",
    "        top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "        all_crop_top_load_dfs.append(top_load_df)\n",
    "\n",
    "    # crop_out_put_df = pd.concat(all_crop_top_load_dfs, axis = 1)\n",
    "    # crop_out_put_df = crop_out_put_df.reindex(sorted(crop_out_put_df.columns), axis=1)\n",
    "    # crop_out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"crop_top_attributes.csv\"), index=False)\n",
    "\n",
    "    return all_top_attr\n",
    "\n",
    "print(crop_pca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining TI (soil, lucl, terrain) and TV (crop inventory) datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting top attributes from computed pca\n",
    "def select_top_attr(all_top_attr, attribute) -> list:\n",
    "    selected_attr = []\n",
    "    top_attr = all_top_attr[attribute]\n",
    "    attrs_list = list(top_attr.values())\n",
    "\n",
    "    for i in range(4):\n",
    "        selected_attr.append(attrs_list[0][i])\n",
    "    for i in range(2):\n",
    "        selected_attr.append(attrs_list[1][i])\n",
    "    return list(set(selected_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new datasets with selected attributes\n",
    "def generate_struct_df() -> pd.DataFrame:\n",
    "    attr_collection = []\n",
    "    struct_pca = structural_pca()\n",
    "    for key in struct_pca.keys():\n",
    "        attributes = select_top_attr(struct_pca, key)\n",
    "        path = os.path.join(directory, \"data\", \"raw_datasets\", \"structural_attributes\", \"140_TI_variables\", f\"leb_{key}.csv\")\n",
    "        stations_list = pd.read_csv(os.path.join(directory,  \"data\",\"raw_datasets\", \"structural_attributes\", \"stations_list.csv\")).stations.tolist()\n",
    "        # input_struct_data = input_struct_data[input_struct_data[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        att_file = pd.read_csv(path)\n",
    "        _133_stations_df = att_file[att_file[\"station_id\"].isin(stations_list)]\n",
    "        _133_stations_df = _133_stations_df[attributes]\n",
    "        attr_collection.append(_133_stations_df)\n",
    "\n",
    "    return pd.concat(attr_collection, axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate crop + struct attributes combined dataset.\n",
    "def generate_struct_crop_df(year:int) -> pd.DataFrame:\n",
    "    crops_pca = crop_pca()\n",
    "\n",
    "    for key in crops_pca.keys():\n",
    "        if year == int(key.split(\"_\")[-1]):\n",
    "            attributes = select_top_attr(crops_pca, key)\n",
    "            path = os.path.join(directory, \"data\", \"raw_datasets\", \"structural_attributes\", \"crop_inventories\", f\"{key}.csv\")\n",
    "            attr_file = pd.read_csv(path)\n",
    "            attr_file = attr_file[attributes]\n",
    "            output_df = pd.concat([generate_struct_df(), attr_file], axis=1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return output_df\n",
    "\n",
    "generate_struct_crop_df(2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature engineer (functional attributes - riverflow metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics to drop.\n",
    "drop_metrics = [\n",
    "    \"Station Name\",\n",
    "    \"Country\",\n",
    "    \"Watershed-Area\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"YR-MaxFlow\",\n",
    "    \"GM-MaxFlow\",\n",
    "    \"NGM-MaxFlow\",\n",
    "    \"YR-MinFlow\",\n",
    "    \"GM-MinFlow\",\n",
    "    \"NGM-MinFlow\",\n",
    "    \"YR-MedianFlow\",\n",
    "    \"GM-MedianFlow\",\n",
    "    \"NGM-MedianFlow\",\n",
    "    \"YR-Q95Flow\",\n",
    "    \"GM-Q95Flow\",\n",
    "    \"NGM-Q95Flow\",\n",
    "    \"YR-Q5Flow\",\n",
    "    \"GM-Q5Flow\",\n",
    "    \"NGM-Q5Flow\",\n",
    "]\n",
    "\n",
    "def functional_pca(drop_metrics: list = drop_metrics):\n",
    "    functional_path = os.path.join(directory, \"data\", \"raw_datasets\", \"functional_attributes\", \"133_riverflow\")\n",
    "    functional_files = glob.glob(f\"{functional_path}/*.csv\")\n",
    "\n",
    "    all_top_load_dfs, all_top_metrics = [], {}\n",
    "\n",
    "    for file in functional_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df = df.set_index(df.columns[0])\n",
    "\n",
    "        # drop metrics and checking for missing values.\n",
    "        functional_df = df.loc[:, ~df.columns.str.startswith(tuple(drop_metrics))]\n",
    "        functional_df = functional_df.fillna(functional_df.median())\n",
    "\n",
    "        # pca analysis start here.\n",
    "        # pca_df = pca.pca_analysis(functional_df)\n",
    "        pca_loadings = pca.loadings(functional_df)\n",
    "\n",
    "        explained_var = pca.explained_variance(functional_df)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_metrics = pca.top_attributes(pca_loadings, 5)\n",
    "        all_top_metrics[file.split(\"/\")[-1].split(\".\")[0]] = top_metrics\n",
    "        new_keys = {'PC1': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC1_{pc1_val}', 'PC2': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC2_{pc2_val}'}\n",
    "        renamed_top_metrics = {new_keys[key]: value for key, value in top_metrics.items()}\n",
    "        yr_top_load_df = pd.DataFrame(renamed_top_metrics)\n",
    "        all_top_load_dfs.append(yr_top_load_df)\n",
    "\n",
    "    # out_put_df = pd.concat(all_top_load_dfs, axis = 1)\n",
    "    # out_put_df =out_put_df.reindex(sorted(out_put_df.columns), axis=1)\n",
    "\n",
    "    return all_top_metrics\n",
    "\n",
    "functional_pca()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca for climate indices\n",
    "def climate_pca():\n",
    "    climate_path = os.path.join(directory, \"data\", \"raw_datasets\", \"climate_indices\")\n",
    "    climate_files = glob.glob(f\"{climate_path}/*.csv\")\n",
    "\n",
    "    all_top_load_dfs, all_top_metrics = [], {}\n",
    "\n",
    "    for file in climate_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns.values[0] = \"station_id\"\n",
    "        df = df.set_index(df.columns[0])\n",
    "\n",
    "        # drop metrics and checking for missing values.\n",
    "        climate_df = df.fillna(df.median())\n",
    "\n",
    "    #     # pca analysis start here.\n",
    "    #     # pca_df = pca.pca_analysis(climate_df)\n",
    "        pca_loadings = pca.loadings(climate_df)\n",
    "\n",
    "        explained_var = pca.explained_variance(climate_df)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_metrics = pca.top_attributes(pca_loadings, 5)\n",
    "        all_top_metrics[file.split(\"/\")[-1].split(\".\")[0]] = top_metrics\n",
    "        new_keys = {'PC1': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC1_{pc1_val}', 'PC2': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC2_{pc2_val}'}\n",
    "        renamed_top_metrics = {new_keys[key]: value for key, value in top_metrics.items()}\n",
    "        yr_top_load_df = pd.DataFrame(renamed_top_metrics)\n",
    "        all_top_load_dfs.append(yr_top_load_df)\n",
    "\n",
    "    # out_put_df = pd.concat(all_top_load_dfs, axis = 1)\n",
    "    # out_put_df =out_put_df.reindex(sorted(out_put_df.columns), axis=1)\n",
    "\n",
    "    return all_top_metrics\n",
    "\n",
    "climate_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new dataset for AP classification (riverflows metrics - year and seasonal).\n",
    "def selected_func_metrics() -> dict:\n",
    "    stations_list = pd.read_csv(\n",
    "        os.path.join(\n",
    "            directory, \"data\", \"raw_datasets\", \"structural_attributes\", \"stations_list.csv\"\n",
    "        )\n",
    "    ).stations.tolist()\n",
    "\n",
    "    func_metrics = {}\n",
    "\n",
    "    for i in range(2011, 2021):\n",
    "        attributes = select_top_attr(functional_pca(), str(i))\n",
    "\n",
    "        functional_path = os.path.join(\n",
    "            directory, \"data\", \"raw_datasets\", \"functional_attributes\", \"133_riverflow\", f'{i}.csv'\n",
    "        )\n",
    "        functional_df = pd.read_csv(functional_path)\n",
    "        functional_df = functional_df[attributes]\n",
    "        functional_df['station_id'] = stations_list\n",
    "        functional_df = functional_df[[\"station_id\"] + [col for col in functional_df.columns if col != \"station_id\"]]\n",
    "        func_metrics[str(i)] = functional_df\n",
    "        dir = os.path.join(output_dir, \"func\")\n",
    "        functional_df.to_csv(dir +  f\"/{i}_func_metrics.csv\", index=False)\n",
    "\n",
    "    return func_metrics\n",
    "\n",
    "selected_func_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new dataset for AP classification (riverflows metrics - climate indices).\n",
    "def selected_func_climate_attrs() -> pd.DataFrame:\n",
    "    for i in tqdm(range(2011, 2021)):\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(\n",
    "                directory,\n",
    "                \"data\",\n",
    "                \"raw_datasets\",\n",
    "                \"climate_indices\",\n",
    "                f\"climate_indices_{str(i)}.csv\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        stations_list = pd.read_csv(\n",
    "        os.path.join(\n",
    "                directory, \"data\", \"raw_datasets\", \"structural_attributes\", \"stations_list.csv\"\n",
    "            )\n",
    "        ).stations.tolist()\n",
    "\n",
    "        stations_list = sorted(stations_list)\n",
    "\n",
    "        df.columns.values[0] = \"station_id\"\n",
    "        df['station_id'] = df['station_id'].apply(lambda val: val.lstrip(\"0\") if val.startswith(\"04\") else val)\n",
    "\n",
    "        climate_df = df[df[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        # sort the climate_df\n",
    "        climate_df = climate_df.sort_values(\"station_id\")\n",
    "        climate_df.index = range(len(climate_df))\n",
    "\n",
    "        attributes = select_top_attr(climate_pca(), f\"climate_indices_{str(i)}\")\n",
    "        climate_df = climate_df[attributes]\n",
    "\n",
    "        output_df = pd.concat([selected_func_metrics()[str(i)], climate_df], axis=1)\n",
    "\n",
    "        output_df = output_df[[\"station_id\"] + [col for col in output_df.columns if col != \"station_id\"]]\n",
    "\n",
    "        # output_df.to_csv(os.path.join(output_dir, \"func_climate\", f\"{i}_func_climate_attrs.csv\"), index=False)\n",
    "        output_df.to_csv(\n",
    "            os.path.join(output_dir, \"func_climate\", f\"{i}_func_climate_attrs.csv\")\n",
    "        )\n",
    "\n",
    "    return output_df\n",
    "\n",
    "selected_func_climate_attrs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new dataset for AP classification (riverflow metrics + structural attributes)\n",
    "def selected_func_struct_attrs() -> dict:\n",
    "    func_struct_attrs = {}\n",
    "    for i in range(2011, 2021):\n",
    "        functional_df = selected_func_metrics()[str(i)]\n",
    "        output_df = pd.concat([generate_struct_crop_df(i), functional_df], axis=1)\n",
    "        output_df = output_df[\n",
    "            [\"station_id\"] + [col for col in output_df.columns if col != \"station_id\"]\n",
    "        ]\n",
    "        func_struct_attrs[str(i)] = output_df\n",
    "\n",
    "    return func_struct_attrs\n",
    "\n",
    "selected_func_struct_attrs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:12<00:00,  7.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate new dataset for AP classification (riverflow metrics + structural attributes + climate indices)\n",
    "def selected_func_struct_climate_attrs() -> pd.DataFrame:\n",
    "    for i in tqdm(range(2011, 2021)):\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(\n",
    "                directory,\n",
    "                \"data\",\n",
    "                \"raw_datasets\",\n",
    "                \"climate_indices\",\n",
    "                f\"climate_indices_{str(i)}.csv\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        stations_list = pd.read_csv(\n",
    "        os.path.join(\n",
    "                directory, \"data\", \"raw_datasets\", \"structural_attributes\", \"stations_list.csv\"\n",
    "            )\n",
    "        ).stations.tolist()\n",
    "\n",
    "        stations_list = sorted(stations_list)\n",
    "\n",
    "        df.columns.values[0] = \"station_id\"\n",
    "        df['station_id'] = df['station_id'].apply(lambda val: val.lstrip(\"0\") if val.startswith(\"04\") else val)\n",
    "\n",
    "        climate_df = df[df[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        # sort the climate_df\n",
    "        climate_df = climate_df.sort_values(\"station_id\")\n",
    "        climate_df.index = range(len(climate_df))\n",
    "\n",
    "        attributes = select_top_attr(climate_pca(), f\"climate_indices_{str(i)}\")\n",
    "        climate_df = climate_df[attributes]\n",
    "\n",
    "        output_df = pd.concat(\n",
    "            [selected_func_struct_attrs()[str(i)], climate_df], axis=1)\n",
    "\n",
    "        output_df = output_df[\n",
    "            [\"station_id\"] + [col for col in output_df.columns if col != \"station_id\"]\n",
    "        ]\n",
    "\n",
    "        output_df.to_csv(os.path.join(output_dir, \"all_attributes\", f\"{i}_func_struct_climate_attrs.csv\"), index=False)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "# print(selected_func_struct_climate_attrs())\n",
    "selected_func_struct_climate_attrs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
