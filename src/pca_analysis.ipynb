{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca import PCA_Analysis\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "# load .env file and pca class\n",
    "load_dotenv()\n",
    "directory = os.getenv(\"path\")\n",
    "output_dir = os.getenv(\"output_path\")\n",
    "pca = PCA_Analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structural attributes (Time-invariant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_pca() -> dict:\n",
    "    struct_path = os.path.join(directory, \"structural_attributes\", \"140_TI_variables\")\n",
    "    stations_list = pd.read_csv(os.path.join(directory, \"structural_attributes\", \"stations_list.csv\")).stations.tolist()\n",
    "\n",
    "\n",
    "    struct_files = os.listdir(struct_path)\n",
    "    struct_files = [file for file in struct_files if file.endswith(\".csv\")]\n",
    "\n",
    "    all_top_load_dfs, all_top_attr = [], {}\n",
    "\n",
    "    for file in struct_files:\n",
    "        attr_type = file.split(\"_\")[1].split(\".\")[0]\n",
    "        input_struct_data = pd.read_csv(os.path.join(struct_path, file))\n",
    "\n",
    "        input_struct_data = input_struct_data[input_struct_data[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        struct_data = pd.concat([input_struct_data[\"station_id\"], input_struct_data.iloc[:, 4:]], axis = 1)\n",
    "        struct_data = struct_data.set_index(\"station_id\")\n",
    "\n",
    "        # data = pca.pca_analysis(struct_data)\n",
    "        loadings = pca.loadings(struct_data)\n",
    "\n",
    "        explained_var = pca.explained_variance(struct_data)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_attr = pca.top_attributes(loadings, 5)\n",
    "        all_top_attr[attr_type] = top_attr\n",
    "        new_keys = {'PC1': f'{attr_type}_PC1_{pc1_val}', 'PC2': f'{attr_type}_PC2_{pc2_val}'}\n",
    "        renamed_top_attr = {new_keys[key]: value for key, value in top_attr.items()}\n",
    "        top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "        all_top_load_dfs.append(top_load_df)\n",
    "    # return  pd.concat(all_top_load_dfs, axis = 1)\n",
    "    # out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"TI_top_attributes.csv\"), index=False)\n",
    "    return all_top_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop inventories (Time-variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_pca() -> dict:\n",
    "    crops_path = os.path.join(directory, \"structural_attributes\", \"crop_inventories\")\n",
    "    crop_inventories_files = glob.glob(f\"{crops_path}/*.csv\")\n",
    "\n",
    "    all_crop_top_load_dfs, all_top_attr  = [], {}\n",
    "\n",
    "    for path in crop_inventories_files:\n",
    "        crop_yr_df = pd.read_csv(path)\n",
    "        crop_yr_df = crop_yr_df.set_index(crop_yr_df.columns[0])\n",
    "\n",
    "        # crop pca analysis\n",
    "        # pca_df = pca.pca_analysis(crop_yr_df)\n",
    "        loadings = pca.loadings(crop_yr_df)\n",
    "\n",
    "        explained_var = pca.explained_variance(crop_yr_df)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_attr = pca.top_attributes(loadings, 5)\n",
    "        all_top_attr[path.split(\"/\")[-1].split(\".\")[0]] = top_attr\n",
    "        new_keys = {'PC1': f'{path.split(\"/\")[-1].split(\".\")[0]}_PC1_{pc1_val}', 'PC2': f'{path.split(\"/\")[-1].split(\".\")[0]}_PC2_{pc2_val}'}\n",
    "        renamed_top_attr = {new_keys[key]: value for key, value in top_attr.items()}\n",
    "        top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "        all_crop_top_load_dfs.append(top_load_df)\n",
    "\n",
    "    crop_out_put_df = pd.concat(all_crop_top_load_dfs, axis = 1)\n",
    "    crop_out_put_df = crop_out_put_df.reindex(sorted(crop_out_put_df.columns), axis=1)\n",
    "    # crop_out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"crop_top_attributes.csv\"), index=False)\n",
    "    return all_top_attr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining TI (soil, lucl, terrain) and TV (crop inventory) datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting top attributes from computed pca\n",
    "def select_top_attr(all_top_attr, attr_type) -> list:\n",
    "    selected_attr = []\n",
    "    top_attr = all_top_attr[attr_type]\n",
    "    attrs_list = list(top_attr.values())\n",
    "\n",
    "    for i in range(4):\n",
    "        selected_attr.append(attrs_list[0][i])\n",
    "    for i in range(2):\n",
    "        selected_attr.append(attrs_list[1][i])\n",
    "    return list(set(selected_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new datasets with selected attributes\n",
    "def generate_struct_df() -> pd.DataFrame:\n",
    "    attr_collection = []\n",
    "    struct_pca = structural_pca()\n",
    "    for key in struct_pca.keys():\n",
    "        attributes = select_top_attr(struct_pca, key)\n",
    "        path = os.path.join(directory, \"structural_attributes\", \"140_TI_variables\", f\"leb_{key}.csv\")\n",
    "        stations_list = pd.read_csv(os.path.join(directory, \"structural_attributes\", \"stations_list.csv\")).stations.tolist()\n",
    "        # input_struct_data = input_struct_data[input_struct_data[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        att_file = pd.read_csv(path)\n",
    "        _133_stations_df = att_file[att_file[\"station_id\"].isin(stations_list)]\n",
    "        _133_stations_df = _133_stations_df[attributes]\n",
    "        attr_collection.append(_133_stations_df)\n",
    "\n",
    "    return pd.concat(attr_collection, axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>std_topsoil_silt</th>\n",
       "      <th>median_topsoil_sand</th>\n",
       "      <th>median_topsoil_silt</th>\n",
       "      <th>std_topsoil_sand</th>\n",
       "      <th>std_topsoil_clay</th>\n",
       "      <th>%water</th>\n",
       "      <th>%cropland</th>\n",
       "      <th>%forest</th>\n",
       "      <th>%grassland</th>\n",
       "      <th>%wetland</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_elevation</th>\n",
       "      <th>std_elevation</th>\n",
       "      <th>median_elevation</th>\n",
       "      <th>max_elevation</th>\n",
       "      <th>tomatoes2011</th>\n",
       "      <th>grapes2011</th>\n",
       "      <th>winterwheat2011</th>\n",
       "      <th>soyabeans2011</th>\n",
       "      <th>oats2011</th>\n",
       "      <th>springwheat2011</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.170080</td>\n",
       "      <td>26</td>\n",
       "      <td>50</td>\n",
       "      <td>15.565180</td>\n",
       "      <td>8.705036</td>\n",
       "      <td>1.392658</td>\n",
       "      <td>68.998906</td>\n",
       "      <td>15.832885</td>\n",
       "      <td>0.437804</td>\n",
       "      <td>0.664702</td>\n",
       "      <td>...</td>\n",
       "      <td>408.804259</td>\n",
       "      <td>62.273206</td>\n",
       "      <td>411</td>\n",
       "      <td>539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.805613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.104138</td>\n",
       "      <td>17</td>\n",
       "      <td>50</td>\n",
       "      <td>12.031182</td>\n",
       "      <td>6.927044</td>\n",
       "      <td>0.057743</td>\n",
       "      <td>84.426076</td>\n",
       "      <td>10.223111</td>\n",
       "      <td>0.280158</td>\n",
       "      <td>0.260911</td>\n",
       "      <td>...</td>\n",
       "      <td>456.647797</td>\n",
       "      <td>18.732971</td>\n",
       "      <td>462</td>\n",
       "      <td>508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.084754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.926256</td>\n",
       "      <td>17</td>\n",
       "      <td>50</td>\n",
       "      <td>13.566371</td>\n",
       "      <td>9.597431</td>\n",
       "      <td>0.549254</td>\n",
       "      <td>80.612989</td>\n",
       "      <td>11.655630</td>\n",
       "      <td>0.220120</td>\n",
       "      <td>0.249661</td>\n",
       "      <td>...</td>\n",
       "      <td>354.024238</td>\n",
       "      <td>36.743025</td>\n",
       "      <td>363</td>\n",
       "      <td>441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.255434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.847713</td>\n",
       "      <td>26</td>\n",
       "      <td>50</td>\n",
       "      <td>13.724722</td>\n",
       "      <td>8.844053</td>\n",
       "      <td>2.085602</td>\n",
       "      <td>74.802225</td>\n",
       "      <td>15.943270</td>\n",
       "      <td>0.799634</td>\n",
       "      <td>1.717514</td>\n",
       "      <td>...</td>\n",
       "      <td>490.430777</td>\n",
       "      <td>14.320739</td>\n",
       "      <td>487</td>\n",
       "      <td>539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.356624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.991387</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "      <td>7.125058</td>\n",
       "      <td>4.134774</td>\n",
       "      <td>0.867342</td>\n",
       "      <td>61.204512</td>\n",
       "      <td>26.616951</td>\n",
       "      <td>0.340075</td>\n",
       "      <td>0.477796</td>\n",
       "      <td>...</td>\n",
       "      <td>385.497491</td>\n",
       "      <td>36.667031</td>\n",
       "      <td>384</td>\n",
       "      <td>506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.329457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2.715522</td>\n",
       "      <td>31</td>\n",
       "      <td>56</td>\n",
       "      <td>3.409538</td>\n",
       "      <td>5.111560</td>\n",
       "      <td>0.341540</td>\n",
       "      <td>20.926085</td>\n",
       "      <td>62.425168</td>\n",
       "      <td>0.744922</td>\n",
       "      <td>4.021247</td>\n",
       "      <td>...</td>\n",
       "      <td>391.824850</td>\n",
       "      <td>93.604877</td>\n",
       "      <td>404</td>\n",
       "      <td>598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064596</td>\n",
       "      <td>0.158610</td>\n",
       "      <td>0.230181</td>\n",
       "      <td>0.033966</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>3.155841</td>\n",
       "      <td>31</td>\n",
       "      <td>51</td>\n",
       "      <td>4.228345</td>\n",
       "      <td>6.100140</td>\n",
       "      <td>0.862061</td>\n",
       "      <td>38.923298</td>\n",
       "      <td>49.756332</td>\n",
       "      <td>0.727606</td>\n",
       "      <td>4.273898</td>\n",
       "      <td>...</td>\n",
       "      <td>465.915367</td>\n",
       "      <td>70.777109</td>\n",
       "      <td>473</td>\n",
       "      <td>630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358078</td>\n",
       "      <td>0.885742</td>\n",
       "      <td>0.077918</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>3.521046</td>\n",
       "      <td>31</td>\n",
       "      <td>51</td>\n",
       "      <td>4.476393</td>\n",
       "      <td>4.893305</td>\n",
       "      <td>0.989909</td>\n",
       "      <td>43.565460</td>\n",
       "      <td>39.755651</td>\n",
       "      <td>0.727843</td>\n",
       "      <td>8.006014</td>\n",
       "      <td>...</td>\n",
       "      <td>404.317641</td>\n",
       "      <td>95.489068</td>\n",
       "      <td>407</td>\n",
       "      <td>630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.567742</td>\n",
       "      <td>1.098605</td>\n",
       "      <td>0.120242</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.280444</td>\n",
       "      <td>23</td>\n",
       "      <td>51</td>\n",
       "      <td>7.571920</td>\n",
       "      <td>3.999675</td>\n",
       "      <td>0.699812</td>\n",
       "      <td>45.486890</td>\n",
       "      <td>28.492574</td>\n",
       "      <td>0.528546</td>\n",
       "      <td>17.236124</td>\n",
       "      <td>...</td>\n",
       "      <td>314.382768</td>\n",
       "      <td>106.358210</td>\n",
       "      <td>278</td>\n",
       "      <td>630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>1.364272</td>\n",
       "      <td>2.435316</td>\n",
       "      <td>0.137531</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2.129150</td>\n",
       "      <td>23</td>\n",
       "      <td>51</td>\n",
       "      <td>3.193725</td>\n",
       "      <td>1.064575</td>\n",
       "      <td>0.893470</td>\n",
       "      <td>29.530264</td>\n",
       "      <td>28.396166</td>\n",
       "      <td>0.930647</td>\n",
       "      <td>19.773589</td>\n",
       "      <td>...</td>\n",
       "      <td>265.739443</td>\n",
       "      <td>46.275710</td>\n",
       "      <td>257</td>\n",
       "      <td>442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>0.807542</td>\n",
       "      <td>1.102855</td>\n",
       "      <td>0.072145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     std_topsoil_silt  median_topsoil_sand  median_topsoil_silt  \\\n",
       "0            8.170080                   26                   50   \n",
       "1            5.104138                   17                   50   \n",
       "2            5.926256                   17                   50   \n",
       "3            7.847713                   26                   50   \n",
       "4            2.991387                   50                   36   \n",
       "..                ...                  ...                  ...   \n",
       "128          2.715522                   31                   56   \n",
       "129          3.155841                   31                   51   \n",
       "130          3.521046                   31                   51   \n",
       "131          7.280444                   23                   51   \n",
       "132          2.129150                   23                   51   \n",
       "\n",
       "     std_topsoil_sand  std_topsoil_clay    %water  %cropland    %forest  \\\n",
       "0           15.565180          8.705036  1.392658  68.998906  15.832885   \n",
       "1           12.031182          6.927044  0.057743  84.426076  10.223111   \n",
       "2           13.566371          9.597431  0.549254  80.612989  11.655630   \n",
       "3           13.724722          8.844053  2.085602  74.802225  15.943270   \n",
       "4            7.125058          4.134774  0.867342  61.204512  26.616951   \n",
       "..                ...               ...       ...        ...        ...   \n",
       "128          3.409538          5.111560  0.341540  20.926085  62.425168   \n",
       "129          4.228345          6.100140  0.862061  38.923298  49.756332   \n",
       "130          4.476393          4.893305  0.989909  43.565460  39.755651   \n",
       "131          7.571920          3.999675  0.699812  45.486890  28.492574   \n",
       "132          3.193725          1.064575  0.893470  29.530264  28.396166   \n",
       "\n",
       "     %grassland   %wetland  ...  mean_elevation  std_elevation  \\\n",
       "0      0.437804   0.664702  ...      408.804259      62.273206   \n",
       "1      0.280158   0.260911  ...      456.647797      18.732971   \n",
       "2      0.220120   0.249661  ...      354.024238      36.743025   \n",
       "3      0.799634   1.717514  ...      490.430777      14.320739   \n",
       "4      0.340075   0.477796  ...      385.497491      36.667031   \n",
       "..          ...        ...  ...             ...            ...   \n",
       "128    0.744922   4.021247  ...      391.824850      93.604877   \n",
       "129    0.727606   4.273898  ...      465.915367      70.777109   \n",
       "130    0.727843   8.006014  ...      404.317641      95.489068   \n",
       "131    0.528546  17.236124  ...      314.382768     106.358210   \n",
       "132    0.930647  19.773589  ...      265.739443      46.275710   \n",
       "\n",
       "     median_elevation  max_elevation  tomatoes2011  grapes2011  \\\n",
       "0                 411            539           0.0    0.000000   \n",
       "1                 462            508           0.0    0.000000   \n",
       "2                 363            441           0.0    0.000000   \n",
       "3                 487            539           0.0    0.000000   \n",
       "4                 384            506           0.0    0.000000   \n",
       "..                ...            ...           ...         ...   \n",
       "128               404            598           0.0    0.064596   \n",
       "129               473            630           0.0    0.000000   \n",
       "130               407            630           0.0    0.000000   \n",
       "131               278            630           0.0    0.000673   \n",
       "132               257            442           0.0    0.007695   \n",
       "\n",
       "     winterwheat2011  soyabeans2011  oats2011  springwheat2011  \n",
       "0           0.000000       9.805613  0.000000         0.000000  \n",
       "1           0.000000      18.084754  0.000000         0.000000  \n",
       "2           0.000000      12.255434  0.000000         0.000000  \n",
       "3           0.000000       7.356624  0.000000         0.000000  \n",
       "4           0.000000      11.329457  0.000000         0.000000  \n",
       "..               ...            ...       ...              ...  \n",
       "128         0.158610       0.230181  0.033966         0.000303  \n",
       "129         0.358078       0.885742  0.077918         0.000000  \n",
       "130         0.567742       1.098605  0.120242         0.000000  \n",
       "131         1.364272       2.435316  0.137531         0.000000  \n",
       "132         0.807542       1.102855  0.072145         0.000000  \n",
       "\n",
       "[133 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate crop + struct attributes combined dataset.\n",
    "def generate_struct_crop_df(year:int) -> pd.DataFrame:\n",
    "    crops_pca = crop_pca()\n",
    "\n",
    "    for key in crops_pca.keys():\n",
    "        if year == int(key.split(\"_\")[-1]):\n",
    "            attributes = select_top_attr(crops_pca, key)\n",
    "            path = os.path.join(directory, \"structural_attributes\", \"crop_inventories\", f\"{key}.csv\")\n",
    "            att_file = pd.read_csv(path)\n",
    "            att_file = att_file[attributes]\n",
    "            output_df = pd.concat([generate_struct_df(), att_file], axis=1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return output_df\n",
    "\n",
    "generate_struct_crop_df(2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def functional_pca():\n",
    "    functional_path = os.path.join(directory, \"functional_attributes\", \"133_riverflow\")\n",
    "    functional_files = glob.glob(f\"{functional_path}/*.csv\")\n",
    "\n",
    "    yr_all_top_load_dfs, all_top_attr = [], {}\n",
    "\n",
    "    for file in functional_files:\n",
    "        functional_df = pd.read_csv(file)\n",
    "        functional_df = functional_df.set_index(functional_df.columns[0])\n",
    "\n",
    "        # func year\n",
    "        yr_functional_df = functional_df.loc[:, functional_df.columns.str.contains('YR')]\n",
    "        yr_remove_list = [\"YR-MaxFlow\", \"YR-MinFlow\",\"YR-MedianFlow\",\"YR-Q95Flow\",\"YR-Q5Flow\"]\n",
    "        yr_functional_df = yr_functional_df.loc[:, ~yr_functional_df.columns.str.startswith(tuple(yr_remove_list))]\n",
    "        yr_functional_df = yr_functional_df.fillna(yr_functional_df.median())\n",
    "\n",
    "        # func year pca analysis\n",
    "        # yr_pca_df = pca.pca_analysis(yr_functional_df)\n",
    "        yr_loadings = pca.loadings(yr_functional_df)\n",
    "\n",
    "        yr_explained_var = pca.explained_variance(yr_functional_df)\n",
    "        yr_pc1_val = round(yr_explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        yr_pc2_val = round(yr_explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        yr_top_attr = pca.top_attributes(yr_loadings, 5)\n",
    "        all_top_attr[file.split(\"/\")[-1].split(\".\")[0]] = yr_top_attr\n",
    "        new_keys = {'PC1': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC1_{yr_pc1_val}', 'PC2': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC2_{yr_pc2_val}'}\n",
    "        renamed_top_attr = {new_keys[key]: value for key, value in yr_top_attr.items()}\n",
    "        yr_top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "        yr_all_top_load_dfs.append(yr_top_load_df)\n",
    "\n",
    "    yr_out_put_df = pd.concat(yr_all_top_load_dfs, axis = 1)\n",
    "    yr_out_put_df = yr_out_put_df.reindex(sorted(yr_out_put_df.columns), axis=1)\n",
    "    # yr_out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"yr_func_top_attributes.csv\"), index=False)\n",
    "    return all_top_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new dataset for AP classification (riverflow + structural attributes)\n",
    "for i in range(2011, 2021):\n",
    "    attributes = select_top_attr(functional_pca(), str(i))\n",
    "    functional_path = os.path.join(directory, \"functional_attributes\", \"133_riverflow\", f\"{i}.csv\")\n",
    "    functional_df = pd.read_csv(functional_path)\n",
    "    functional_df = functional_df[attributes]\n",
    "    output_df = pd.concat([generate_struct_crop_df(i), functional_df], axis=1)\n",
    "    stations_list = pd.read_csv(os.path.join(directory, \"structural_attributes\", \"stations_list.csv\")).stations.tolist()\n",
    "    output_df = pd.concat([pd.DataFrame(stations_list, columns=[\"station_id\"]), output_df], axis=1).set_index(\"station_id\")\n",
    "    output_df.to_csv(output_dir + f\"/{i}_struct_flow.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year and seasonal functional metrics test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC1: 0.39, PC2: 0.21\n",
      "top_attr {'PC1': ['Specific-NGM-MedianFlow-2011', 'Specific-YR-MedianFlow-2011', 'Specific-GM-MedianFlow-2011', 'Specific-NGM-MinFlow-2011', 'Specific-GM-Q95-2011'], 'PC2': ['NGM-CVQ-2011', 'YR-CVQ-2011', 'YR-RBI-2011', 'GM-RBI-2011', 'NGM-RBI-2011']}\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"/Users/mugisha/Desktop/2011.csv\"\n",
    "struct_data = pd.read_csv(file_path)\n",
    "\n",
    "# check if there is any missing value struct_data\n",
    "# replace missing values with median\n",
    "struct_data = struct_data.fillna(struct_data.median())\n",
    "\n",
    "loadings = pca.loadings(struct_data)\n",
    "\n",
    "explained_var = pca.explained_variance(struct_data)\n",
    "\n",
    "\n",
    "pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "top_attr = pca.top_attributes(loadings, 5)\n",
    "\n",
    "print(f\"PC1: {pc1_val}, PC2: {pc2_val}\")\n",
    "print(f\"top_attr {top_attr}\")\n",
    "\n",
    "\n",
    "# all_top_attr[attr_type] = top_attr\n",
    "# new_keys = {'PC1': f'{attr_type}_PC1_{pc1_val}', 'PC2': f'{attr_type}_PC2_{pc2_val}'}\n",
    "# renamed_top_attr = {new_keys[key]: value for key, value in top_attr.items()}\n",
    "# top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "# all_top_load_dfs.append(top_load_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functional archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional_path = os.path.join(directory, \"functional_attributes/133_riverflow\")\n",
    "# functional_files = glob.glob(f\"{functional_path}/*.csv\")\n",
    "\n",
    "# yr_func_dict, szn_func_dict = {}, {}\n",
    "# yr_all_top_load_dfs, szn_all_top_load_dfs = [], []\n",
    "\n",
    "# for file in functional_files:\n",
    "#     functional_df = pd.read_csv(file)\n",
    "#     functional_df = functional_df.set_index(functional_df.columns[0])\n",
    "\n",
    "#     # func year\n",
    "#     yr_functional_df = functional_df.loc[:, functional_df.columns.str.contains('YR')]\n",
    "#     yr_remove_list = [\"YR-MaxFlow\", \"YR-MinFlow\",\"YR-MedianFlow\",\"YR-Q95Flow\",\"YR-Q5Flow\"]\n",
    "#     yr_functional_df = yr_functional_df.loc[:, ~yr_functional_df.columns.str.startswith(tuple(yr_remove_list))]\n",
    "#     yr_functional_df = yr_functional_df.fillna(yr_functional_df.median())\n",
    "\n",
    "#     # func seasonal\n",
    "#     szn_functional_df = functional_df.loc[:, functional_df.columns.str.contains('GM|NGM')]\n",
    "#     szn_remove_list = [\"GM-MaxFlow\", \"GM-MinFlow\",\"GM-MedianFlow\",\"GM-Q95Flow\",\"GM-Q5Flow\",\n",
    "#                         \"NGM-MaxFlow\", \"NGM-MinFlow\",\"NGM-MedianFlow\",\"NGM-Q95Flow\",\"NGM-Q5Flow\"]\n",
    "#     szn_functional_df = szn_functional_df.loc[:, ~szn_functional_df.columns.str.startswith(tuple(szn_remove_list))]\n",
    "#     szn_functional_df = szn_functional_df.fillna(szn_functional_df.median())\n",
    "\n",
    "#     # func year pca analysis\n",
    "#     yr_pca_df = pca.pca_analysis(yr_functional_df)\n",
    "#     yr_loadings = pca.loadings(yr_functional_df)\n",
    "\n",
    "#     yr_explained_var = pca.explained_variance(yr_functional_df)\n",
    "#     yr_pc1_val = round(yr_explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "#     yr_pc2_val = round(yr_explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "#     yr_top_attr = pca.top_attributes(yr_loadings, 5)\n",
    "#     new_keys = {'PC1': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC1_{yr_pc1_val}', 'PC2': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC2_{yr_pc2_val}'}\n",
    "#     renamed_top_attr = {new_keys[key]: value for key, value in yr_top_attr.items()}\n",
    "#     yr_top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "#     yr_all_top_load_dfs.append(yr_top_load_df)\n",
    "\n",
    "#     # func seasonal pca analysis\n",
    "#     szn_pca_df = pca.pca_analysis(szn_functional_df)\n",
    "#     szn_loadings = pca.loadings(szn_functional_df)\n",
    "\n",
    "#     szn_explained_var = pca.explained_variance(szn_functional_df)\n",
    "#     szn_pc1_val = round(szn_explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "#     szn_pc2_val = round(szn_explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "#     szn_top_attr = pca.top_attributes(szn_loadings, 5)\n",
    "#     new_keys = {'PC1': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC1_{szn_pc1_val}', 'PC2': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC2_{szn_pc2_val}'}\n",
    "#     renamed_top_attr = {new_keys[key]: value for key, value in szn_top_attr.items()}\n",
    "#     szn_top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "#     szn_all_top_load_dfs.append(szn_top_load_df)\n",
    "\n",
    "# yr_out_put_df = pd.concat(yr_all_top_load_dfs, axis = 1)\n",
    "# yr_out_put_df = yr_out_put_df.reindex(sorted(yr_out_put_df.columns), axis=1)\n",
    "# yr_out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"yr_func_top_attributes.csv\"), index=False)\n",
    "\n",
    "# szn_out_put_df = pd.concat(szn_all_top_load_dfs, axis = 1)\n",
    "# szn_out_put_df = szn_out_put_df.reindex(sorted(szn_out_put_df.columns), axis=1)\n",
    "# szn_out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"szn_func_top_attributes.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
