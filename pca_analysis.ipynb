{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca import PCA_Analysis\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load .env file and pca class\n",
    "load_dotenv()\n",
    "directory = os.getenv(\"path\")\n",
    "output_dir = os.getenv(\"output_dir\")\n",
    "pca = PCA_Analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting top attributes from computed pca\n",
    "def select_top_attr(all_top_attr, attribute) -> list:\n",
    "    selected_attr = []\n",
    "    top_attr = all_top_attr[attribute]\n",
    "    attrs_list = list(top_attr.values())\n",
    "\n",
    "    for i in range(4):\n",
    "        selected_attr.append(attrs_list[0][i])\n",
    "    for i in range(2):\n",
    "        selected_attr.append(attrs_list[1][i])\n",
    "    return list(set(selected_attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structural attributes (Time-invariant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'soil': {'PC1': ['mean_topsoil_sand',\n",
       "   'max_topsoil_sand',\n",
       "   'median_topsoil_sand',\n",
       "   'mean_topsoil_silt',\n",
       "   'min_topsoil_silt'],\n",
       "  'PC2': ['std_topsoil_clay',\n",
       "   'min_topsoil_sand',\n",
       "   'std_topsoil_sand',\n",
       "   'std_topsoil_silt',\n",
       "   'median_soil_depth']},\n",
       " 'surficial': {'PC1': ['Alluvium',\n",
       "   'Kames and esker',\n",
       "   'Undefined',\n",
       "   'Lacustrine',\n",
       "   'End moraine'],\n",
       "  'PC2': ['Ground moraine',\n",
       "   'End moraine',\n",
       "   'Lacustrine',\n",
       "   'Undifferentiated',\n",
       "   'Outwash and dunes']},\n",
       " 'lulcstats': {'PC1': ['%cropland',\n",
       "   '%grassland',\n",
       "   '%wetland',\n",
       "   '%water',\n",
       "   '%urban'],\n",
       "  'PC2': ['%forest', '%grassland', '%water', '%urban', '%wetland']},\n",
       " 'terrain': {'PC1': ['max_elevation',\n",
       "   'mean_elevation',\n",
       "   'median_elevation',\n",
       "   'range_elevation',\n",
       "   'std_elevation'],\n",
       "  'PC2': ['min_elevation',\n",
       "   'max_slope',\n",
       "   'range_elevation',\n",
       "   'median_elevation',\n",
       "   'mean_elevation']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def structural_pca() -> dict:\n",
    "    # struct_path = os.path.join(directory, \"data\",\"raw_datasets\", \"structural_attributes\", \"140_TI_variables\")\n",
    "    struct_path = os.path.join(directory, \"computed_data\", \"TI_variables\")\n",
    "\n",
    "    stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "\n",
    "    struct_files = os.listdir(struct_path)\n",
    "    struct_files = [file for file in struct_files if file.endswith(\".csv\")]\n",
    "\n",
    "    all_top_load_dfs, all_top_attr = [], {}\n",
    "\n",
    "    for file in struct_files:\n",
    "        attr_type = file.split(\"_\")[1].split(\".\")[0]\n",
    "        input_struct_data = pd.read_csv(os.path.join(struct_path, file))\n",
    "\n",
    "        input_struct_data = input_struct_data[input_struct_data[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        struct_data = pd.concat([input_struct_data[\"station_id\"], input_struct_data.iloc[:, 4:]], axis = 1)\n",
    "        struct_data = struct_data.set_index(\"station_id\")\n",
    "\n",
    "        loadings = pca.loadings(struct_data)\n",
    "\n",
    "        explained_var = pca.explained_variance(struct_data)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_attr = pca.top_attributes(loadings, 5)\n",
    "        all_top_attr[attr_type] = top_attr\n",
    "        new_keys = {'PC1': f'{attr_type}_PC1_{pc1_val}', 'PC2': f'{attr_type}_PC2_{pc2_val}'}\n",
    "        renamed_top_attr = {new_keys[key]: value for key, value in top_attr.items()}\n",
    "        top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "        all_top_load_dfs.append(top_load_df)\n",
    "    # return  pd.concat(all_top_load_dfs, axis = 1)\n",
    "    # out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"TI_top_attributes.csv\"), index=False)\n",
    "    return all_top_attr\n",
    "\n",
    "structural_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_topsoil_silt</th>\n",
       "      <th>median_topsoil_sand</th>\n",
       "      <th>min_topsoil_sand</th>\n",
       "      <th>std_topsoil_clay</th>\n",
       "      <th>mean_topsoil_sand</th>\n",
       "      <th>max_topsoil_sand</th>\n",
       "      <th>Undefined</th>\n",
       "      <th>End moraine</th>\n",
       "      <th>Ground moraine</th>\n",
       "      <th>Alluvium</th>\n",
       "      <th>...</th>\n",
       "      <th>%water</th>\n",
       "      <th>%grassland</th>\n",
       "      <th>%wetland</th>\n",
       "      <th>%cropland</th>\n",
       "      <th>median_elevation</th>\n",
       "      <th>mean_elevation</th>\n",
       "      <th>min_elevation</th>\n",
       "      <th>range_elevation</th>\n",
       "      <th>max_elevation</th>\n",
       "      <th>max_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.782889</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>8.705036</td>\n",
       "      <td>33.512144</td>\n",
       "      <td>60</td>\n",
       "      <td>0.916557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.841742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.392658</td>\n",
       "      <td>0.437804</td>\n",
       "      <td>0.664702</td>\n",
       "      <td>68.998906</td>\n",
       "      <td>411</td>\n",
       "      <td>408.804259</td>\n",
       "      <td>259</td>\n",
       "      <td>280</td>\n",
       "      <td>539</td>\n",
       "      <td>31.821241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.790384</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>6.927044</td>\n",
       "      <td>22.208381</td>\n",
       "      <td>50</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.796098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057743</td>\n",
       "      <td>0.280158</td>\n",
       "      <td>0.260911</td>\n",
       "      <td>84.426076</td>\n",
       "      <td>462</td>\n",
       "      <td>456.647797</td>\n",
       "      <td>403</td>\n",
       "      <td>105</td>\n",
       "      <td>508</td>\n",
       "      <td>13.121887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.808674</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>9.597431</td>\n",
       "      <td>28.581898</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.611159</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549254</td>\n",
       "      <td>0.220120</td>\n",
       "      <td>0.249661</td>\n",
       "      <td>80.612989</td>\n",
       "      <td>363</td>\n",
       "      <td>354.024238</td>\n",
       "      <td>247</td>\n",
       "      <td>194</td>\n",
       "      <td>441</td>\n",
       "      <td>27.372448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.330775</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>8.844053</td>\n",
       "      <td>27.425381</td>\n",
       "      <td>60</td>\n",
       "      <td>2.016131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.117491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.085602</td>\n",
       "      <td>0.799634</td>\n",
       "      <td>1.717514</td>\n",
       "      <td>74.802225</td>\n",
       "      <td>487</td>\n",
       "      <td>490.430777</td>\n",
       "      <td>437</td>\n",
       "      <td>102</td>\n",
       "      <td>539</td>\n",
       "      <td>16.726595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.174353</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>4.134774</td>\n",
       "      <td>49.640914</td>\n",
       "      <td>60</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.288441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867342</td>\n",
       "      <td>0.340075</td>\n",
       "      <td>0.477796</td>\n",
       "      <td>61.204512</td>\n",
       "      <td>384</td>\n",
       "      <td>385.497491</td>\n",
       "      <td>295</td>\n",
       "      <td>211</td>\n",
       "      <td>506</td>\n",
       "      <td>25.869267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>54.815661</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>5.111560</td>\n",
       "      <td>29.625678</td>\n",
       "      <td>41</td>\n",
       "      <td>0.088710</td>\n",
       "      <td>33.681957</td>\n",
       "      <td>36.586062</td>\n",
       "      <td>3.663547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341540</td>\n",
       "      <td>0.744922</td>\n",
       "      <td>4.021247</td>\n",
       "      <td>20.926085</td>\n",
       "      <td>404</td>\n",
       "      <td>391.824850</td>\n",
       "      <td>183</td>\n",
       "      <td>415</td>\n",
       "      <td>598</td>\n",
       "      <td>27.470636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>52.721364</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>6.100140</td>\n",
       "      <td>27.720180</td>\n",
       "      <td>33</td>\n",
       "      <td>0.110957</td>\n",
       "      <td>25.047208</td>\n",
       "      <td>48.042152</td>\n",
       "      <td>3.170166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862061</td>\n",
       "      <td>0.727606</td>\n",
       "      <td>4.273898</td>\n",
       "      <td>38.923298</td>\n",
       "      <td>473</td>\n",
       "      <td>465.915367</td>\n",
       "      <td>292</td>\n",
       "      <td>338</td>\n",
       "      <td>630</td>\n",
       "      <td>35.190132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>50.526021</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>4.893305</td>\n",
       "      <td>29.071465</td>\n",
       "      <td>33</td>\n",
       "      <td>0.264663</td>\n",
       "      <td>31.018234</td>\n",
       "      <td>49.985005</td>\n",
       "      <td>3.230700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989909</td>\n",
       "      <td>0.727843</td>\n",
       "      <td>8.006014</td>\n",
       "      <td>43.565460</td>\n",
       "      <td>407</td>\n",
       "      <td>404.317641</td>\n",
       "      <td>265</td>\n",
       "      <td>365</td>\n",
       "      <td>630</td>\n",
       "      <td>35.190132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>52.532807</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>3.999675</td>\n",
       "      <td>26.028401</td>\n",
       "      <td>33</td>\n",
       "      <td>0.066045</td>\n",
       "      <td>32.012867</td>\n",
       "      <td>57.727028</td>\n",
       "      <td>5.689437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699812</td>\n",
       "      <td>0.528546</td>\n",
       "      <td>17.236124</td>\n",
       "      <td>45.486890</td>\n",
       "      <td>278</td>\n",
       "      <td>314.382768</td>\n",
       "      <td>175</td>\n",
       "      <td>455</td>\n",
       "      <td>630</td>\n",
       "      <td>35.190132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>50.609521</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>1.064575</td>\n",
       "      <td>23.585719</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893470</td>\n",
       "      <td>0.930647</td>\n",
       "      <td>19.773589</td>\n",
       "      <td>29.530264</td>\n",
       "      <td>257</td>\n",
       "      <td>265.739443</td>\n",
       "      <td>177</td>\n",
       "      <td>265</td>\n",
       "      <td>442</td>\n",
       "      <td>18.180233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_topsoil_silt  median_topsoil_sand  min_topsoil_sand  \\\n",
       "0            44.782889                   26                17   \n",
       "1            47.790384                   17                17   \n",
       "2            46.808674                   17                17   \n",
       "3            50.330775                   26                17   \n",
       "4            36.174353                   50                17   \n",
       "..                 ...                  ...               ...   \n",
       "128          54.815661                   31                23   \n",
       "129          52.721364                   31                23   \n",
       "130          50.526021                   31                23   \n",
       "131          52.532807                   23                11   \n",
       "132          50.609521                   23                23   \n",
       "\n",
       "     std_topsoil_clay  mean_topsoil_sand  max_topsoil_sand  Undefined  \\\n",
       "0            8.705036          33.512144                60   0.916557   \n",
       "1            6.927044          22.208381                50   0.009040   \n",
       "2            9.597431          28.581898                64   0.000000   \n",
       "3            8.844053          27.425381                60   2.016131   \n",
       "4            4.134774          49.640914                60   0.003601   \n",
       "..                ...                ...               ...        ...   \n",
       "128          5.111560          29.625678                41   0.088710   \n",
       "129          6.100140          27.720180                33   0.110957   \n",
       "130          4.893305          29.071465                33   0.264663   \n",
       "131          3.999675          26.028401                33   0.066045   \n",
       "132          1.064575          23.585719                41   0.000000   \n",
       "\n",
       "     End moraine  Ground moraine  Alluvium  ...    %water  %grassland  \\\n",
       "0       0.000000       70.841742  0.000000  ...  1.392658    0.437804   \n",
       "1       0.000000       94.796098  0.000000  ...  0.057743    0.280158   \n",
       "2       0.000000       62.611159  0.240400  ...  0.549254    0.220120   \n",
       "3       0.000000       74.117491  0.000000  ...  2.085602    0.799634   \n",
       "4       0.000000       62.288441  0.000000  ...  0.867342    0.340075   \n",
       "..           ...             ...       ...  ...       ...         ...   \n",
       "128    33.681957       36.586062  3.663547  ...  0.341540    0.744922   \n",
       "129    25.047208       48.042152  3.170166  ...  0.862061    0.727606   \n",
       "130    31.018234       49.985005  3.230700  ...  0.989909    0.727843   \n",
       "131    32.012867       57.727028  5.689437  ...  0.699812    0.528546   \n",
       "132     0.000000        0.000000  0.000000  ...  0.893470    0.930647   \n",
       "\n",
       "      %wetland  %cropland  median_elevation  mean_elevation  min_elevation  \\\n",
       "0     0.664702  68.998906               411      408.804259            259   \n",
       "1     0.260911  84.426076               462      456.647797            403   \n",
       "2     0.249661  80.612989               363      354.024238            247   \n",
       "3     1.717514  74.802225               487      490.430777            437   \n",
       "4     0.477796  61.204512               384      385.497491            295   \n",
       "..         ...        ...               ...             ...            ...   \n",
       "128   4.021247  20.926085               404      391.824850            183   \n",
       "129   4.273898  38.923298               473      465.915367            292   \n",
       "130   8.006014  43.565460               407      404.317641            265   \n",
       "131  17.236124  45.486890               278      314.382768            175   \n",
       "132  19.773589  29.530264               257      265.739443            177   \n",
       "\n",
       "     range_elevation  max_elevation  max_slope  \n",
       "0                280            539  31.821241  \n",
       "1                105            508  13.121887  \n",
       "2                194            441  27.372448  \n",
       "3                102            539  16.726595  \n",
       "4                211            506  25.869267  \n",
       "..               ...            ...        ...  \n",
       "128              415            598  27.470636  \n",
       "129              338            630  35.190132  \n",
       "130              365            630  35.190132  \n",
       "131              455            630  35.190132  \n",
       "132              265            442  18.180233  \n",
       "\n",
       "[133 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate new datasets with selected attributes\n",
    "def generate_struct_df() -> pd.DataFrame:\n",
    "    attr_collection = []\n",
    "    struct_pca = structural_pca()\n",
    "    for key in struct_pca.keys():\n",
    "        attributes = select_top_attr(struct_pca, key)\n",
    "        path = os.path.join(\n",
    "            directory,\n",
    "            \"computed_data\",\n",
    "            \"TI_variables\",\n",
    "            f\"leb_{key}.csv\",\n",
    "        )\n",
    "        stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "        # input_struct_data = input_struct_data[input_struct_data[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        att_file = pd.read_csv(path)\n",
    "        _133_stations_df = att_file[att_file[\"station_id\"].isin(stations_list)]\n",
    "        _133_stations_df = _133_stations_df[attributes]\n",
    "        attr_collection.append(_133_stations_df)\n",
    "\n",
    "    return pd.concat(attr_collection, axis=1).reset_index(drop=True)\n",
    "\n",
    "generate_struct_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop inventories (Time-variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crop_inventory_2012': {'PC1': ['grapes2012', 'springwheat2012', 'oats2012', 'tomatoes2012', 'sod2012'], 'PC2': ['soyabeans2012', 'winterwheat2012', 'tomatoes2012', 'corn2012', 'springwheat2012']}, 'crop_inventory_2013': {'PC1': ['grapes2013', 'springwheat2013', 'tomatoes2013', 'fallow2013', 'winterwheat2013'], 'PC2': ['soyabeans2013', 'corn2013', 'winterwheat2013', 'fallow2013', 'tomatoes2013']}, 'crop_inventory_2011': {'PC1': ['oats2011', 'springwheat2011', 'grapes2011', 'tomatoes2011', 'peas2011'], 'PC2': ['soyabeans2011', 'winterwheat2011', 'corn2011', 'tomatoes2011', 'fallow2011']}, 'crop_inventory_2014': {'PC1': ['springwheat2014', 'grapes2014', 'winterwheat2014', 'tomatoes2014', 'potatoes2014'], 'PC2': ['soyabeans2014', 'corn2014', 'fallow2014', 'winterwheat2014', 'peas2014']}, 'crop_inventory_2015': {'PC1': ['tomatoes2015', 'grapes2015', 'springwheat2015', 'potatoes2015', 'winterwheat2015'], 'PC2': ['potatoes2015', 'winterwheat2015', 'grapes2015', 'soyabeans2015', 'canola2015']}, 'crop_inventory_2017': {'PC1': ['winterwheat2017', 'corn2017', 'fallow2017', 'soyabeans2017', 'grapes2017'], 'PC2': ['tomatoes2017', 'canola2017', 'soyabeans2017', 'corn2017', 'oats2017']}, 'crop_inventory_2016': {'PC1': ['springwheat2016', 'canola2016', 'oats2016', 'potatoes2016', 'winterwheat2016'], 'PC2': ['corn2016', 'winterwheat2016', 'soyabeans2016', 'fallow2016', 'grapes2016']}, 'crop_inventory_2018': {'PC1': ['winterwheat2018', 'corn2018', 'oats2018', 'springwheat2018', 'beans2018'], 'PC2': ['canola2018', 'oats2018', 'springwheat2018', 'corn2018', 'soyabeans2018']}, 'crop_inventory_2019': {'PC1': ['winterwheat2019', 'corn2019', 'springwheat2019', 'soyabeans2019', 'canola2019'], 'PC2': ['canola2019', 'oats2019', 'soyabeans2019', 'springwheat2019', 'corn2019']}, 'crop_inventory_2020': {'PC1': ['springwheat2020', 'canola2020', 'oats2020', 'potatoes2020', 'winterwheat2020'], 'PC2': ['corn2020', 'winterwheat2020', 'tomatoes2020', 'fallow2020', 'beans2020']}}\n"
     ]
    }
   ],
   "source": [
    "def crop_pca() -> dict:\n",
    "    crops_path = os.path.join(directory, \"computed_data\", \"crop_data\")\n",
    "\n",
    "    crop_inventories_files = glob.glob(f\"{crops_path}/*.csv\")\n",
    "\n",
    "    all_crop_top_load_dfs, all_top_attr  = [], {}\n",
    "\n",
    "    for path in crop_inventories_files:\n",
    "        crop_yr_df = pd.read_csv(path)\n",
    "        crop_yr_df = crop_yr_df.set_index(crop_yr_df.columns[0])\n",
    "\n",
    "        # crop pca analysis\n",
    "        # pca_df = pca.pca_analysis(crop_yr_df)\n",
    "        loadings = pca.loadings(crop_yr_df)\n",
    "\n",
    "        explained_var = pca.explained_variance(crop_yr_df)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_attr = pca.top_attributes(loadings, 5)\n",
    "        all_top_attr[path.split(\"/\")[-1].split(\".\")[0]] = top_attr\n",
    "        new_keys = {'PC1': f'{path.split(\"/\")[-1].split(\".\")[0]}_PC1_{pc1_val}', 'PC2': f'{path.split(\"/\")[-1].split(\".\")[0]}_PC2_{pc2_val}'}\n",
    "        renamed_top_attr = {new_keys[key]: value for key, value in top_attr.items()}\n",
    "        top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "        all_crop_top_load_dfs.append(top_load_df)\n",
    "\n",
    "    # crop_out_put_df = pd.concat(all_crop_top_load_dfs, axis = 1)\n",
    "    # crop_out_put_df = crop_out_put_df.reindex(sorted(crop_out_put_df.columns), axis=1)\n",
    "    # crop_out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"crop_top_attributes.csv\"), index=False)\n",
    "\n",
    "    return all_top_attr\n",
    "\n",
    "print(crop_pca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining TI (soil, lucl, terrain) and TV (crop inventory) datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to select top attributes for year variant attributes.\n",
    "def selectTopAtttributesForYear(datatype: dict) -> list:\n",
    "    pca_selected_attr = []\n",
    "    for _,val in datatype.items():\n",
    "        yearTopAttributes, topAttributes = [], []\n",
    "        for _,v in val.items():\n",
    "            yearTopAttributes.append(v)\n",
    "\n",
    "        for i in range(4):\n",
    "            topAttributes.append(yearTopAttributes[0][i])\n",
    "        for i in range(2):\n",
    "            topAttributes.append(yearTopAttributes[1][i])\n",
    "\n",
    "        pca_selected_attr.append(list(set(topAttributes)))\n",
    "\n",
    "    # selected_attr = []\n",
    "    counter = {}\n",
    "    for arr_attrs in pca_selected_attr:\n",
    "        for attr in arr_attrs:\n",
    "            attr = attr.split(\"2\")[0]\n",
    "            if attr in counter:\n",
    "                counter[attr] += 1\n",
    "            else:\n",
    "                counter[attr] = 1\n",
    "\n",
    "    count, selected_attr = 0, set()\n",
    "    count_values = sorted(list(counter.values()), reverse=True)\n",
    "    # print(\"counter_values\", count_values)\n",
    "    lookup_counter = min(count_values[:5])\n",
    "\n",
    "    for key, value in counter.items():\n",
    "        if value >= lookup_counter:\n",
    "            count += 1\n",
    "            selected_attr.add(key)\n",
    "\n",
    "    return list(selected_attr)\n",
    "\n",
    "# testing:\n",
    "# crops_data = crop_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_topsoil_silt', 'median_topsoil_sand', 'min_topsoil_sand',\n",
       "       'std_topsoil_clay', 'mean_topsoil_sand', 'max_topsoil_sand',\n",
       "       'Undefined', 'End moraine', 'Ground moraine', 'Alluvium', 'Lacustrine',\n",
       "       'Kames and esker', '%forest', '%water', '%grassland', '%wetland',\n",
       "       '%cropland', 'median_elevation', 'mean_elevation', 'min_elevation',\n",
       "       'range_elevation', 'max_elevation', 'max_slope', 'tomatoes', 'oats',\n",
       "       'soyabeans', 'winterwheat', 'springwheat', 'corn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate crop + struct attributes combined dataset.\n",
    "def generate_struct_crop_df(year:int) -> pd.DataFrame:\n",
    "    crops_pca = crop_pca()\n",
    "    selectedAttributesCrops = selectTopAtttributesForYear(crops_pca)\n",
    "    selectedAttributesCropsWithYear = [0]*len(selectedAttributesCrops)\n",
    "\n",
    "    for i in range(len(selectedAttributesCrops)):\n",
    "        selectedAttributesCropsWithYear[i] = f\"{selectedAttributesCrops[i]}{year}\"\n",
    "\n",
    "    for key in crops_pca.keys():\n",
    "        if year == int(key.split(\"_\")[-1]):\n",
    "            # attributes = select_top_attr(crops_pca, key)\n",
    "            path = os.path.join(directory, \"computed_data\",\"crop_data\", f\"{key}.csv\")\n",
    "            attr_file = pd.read_csv(path)\n",
    "            attr_file = attr_file[selectedAttributesCropsWithYear]\n",
    "            attr_file.columns = selectedAttributesCrops\n",
    "\n",
    "            output_df = pd.concat([generate_struct_df(), attr_file], axis=1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return output_df\n",
    "\n",
    "generate_struct_crop_df(2015).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature engineer (functional attributes - riverflow metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metrics2013': {'PC1': ['YR-CVQ-2013', 'NGM-CVQ-2013', 'NGM-RBI-2013', 'GM-CVQ-2013', 'YR-RBI-2013'], 'PC2': ['Specific-NGM-MedianFlow-2013', 'Specific-YR-MedianFlow-2013', 'Specific-GM-Q95-2013', 'Specific-YR-Q95-2013', 'Specific-YR-MaxFlow-2013']}, 'metrics2012': {'PC1': ['YR-CVQ-2012', 'NGM-CVQ-2012', 'YR-RBI-2012', 'NGM-RBI-2012', 'GM-RBI-2012'], 'PC2': ['Specific-NGM-MedianFlow-2012', 'Specific-YR-MedianFlow-2012', 'Specific-YR-Q95-2012', 'Specific-GM-Q95-2012', 'NGM-DOY-MaxFlow-2012']}, 'metrics2011': {'PC1': ['YR-CVQ-2011', 'NGM-CVQ-2011', 'GM-CVQ-2011', 'GM-RBI-2011', 'YR-RBI-2011'], 'PC2': ['Specific-YR-MedianFlow-2011', 'Specific-GM-MedianFlow-2011', 'Specific-NGM-MedianFlow-2011', 'YR-DOY-MaxFlow-2011', 'NGM-DOY-MaxFlow-2011']}, 'metrics2015': {'PC1': ['YR-CVQ-2015', 'NGM-CVQ-2015', 'GM-CVQ-2015', 'Specific-GM-Q5-2015', 'Specific-YR-Q5-2015'], 'PC2': ['Specific-NGM-Q5-2015', 'Specific-NGM-Q95-2015', 'Specific-NGM-MedianFlow-2015', 'Specific-YR-MedianFlow-2015', 'Specific-YR-Q95-2015']}, 'metrics2014': {'PC1': ['YR-CVQ-2014', 'NGM-CVQ-2014', 'Specific-YR-Q5-2014', 'GM-CVQ-2014', 'Specific-GM-Q5-2014'], 'PC2': ['Specific-YR-Q95-2014', 'Specific-YR-MaxFlow-2014', 'Specific-NGM-Q95-2014', 'Specific-NGM-Q5-2014', 'Specific-NGM-MaxFlow-2014']}, 'metrics2016': {'PC1': ['YR-CVQ-2016', 'NGM-CVQ-2016', 'Specific-NGM-MinFlow-2016', 'Specific-YR-Q5-2016', 'NGM-RBI-2016'], 'PC2': ['Specific-NGM-Q5-2016', 'Specific-NGM-Q95-2016', 'Specific-YR-Q95-2016', 'Specific-NGM-MaxFlow-2016', 'Specific-YR-MaxFlow-2016']}, 'metrics2017': {'PC1': ['YR-CVQ-2017', 'NGM-CVQ-2017', 'YR-RBI-2017', 'NGM-RBI-2017', 'GM-RBI-2017'], 'PC2': ['Specific-YR-Q95-2017', 'Specific-YR-MedianFlow-2017', 'Specific-NGM-Q5-2017', 'Specific-NGM-Q95-2017', 'Specific-GM-MedianFlow-2017']}, 'metrics2019': {'PC1': ['YR-CVQ-2019', 'NGM-CVQ-2019', 'GM-CVQ-2019', 'NGM-RBI-2019', 'YR-RBI-2019'], 'PC2': ['GM-DOY-MaxFlow-2019', 'Specific-GM-MedianFlow-2019', 'Specific-YR-Q95-2019', 'Specific-GM-Q95-2019', 'Specific-YR-MedianFlow-2019']}, 'metrics2018': {'PC1': ['Specific-YR-Q5-2018', 'YR-RBI-2018', 'NGM-RBI-2018', 'Specific-YR-MinFlow-2018', 'YR-CVQ-2018'], 'PC2': ['Specific-NGM-Q95-2018', 'Specific-NGM-Q5-2018', 'Specific-NGM-MedianFlow-2018', 'Specific-YR-Q95-2018', 'Specific-YR-MedianFlow-2018']}, 'metrics2020': {'PC1': ['NGM-RBI-2020', 'Specific-GM-Q5-2020', 'Specific-YR-Q5-2020', 'Specific-GM-MinFlow-2020', 'Specific-YR-MinFlow-2020'], 'PC2': ['Specific-YR-Q95-2020', 'Specific-YR-MedianFlow-2020', 'Specific-GM-Q95-2020', 'Specific-NGM-MedianFlow-2020', 'GM-DOY-MaxFlow-2020']}}\n"
     ]
    }
   ],
   "source": [
    "# metrics to drop.\n",
    "drop_metrics = [\n",
    "    \"Station Name\",\n",
    "    \"Country\",\n",
    "    \"Watershed-Area\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"YR-MaxFlow\",\n",
    "    \"GM-MaxFlow\",\n",
    "    \"NGM-MaxFlow\",\n",
    "    \"YR-MinFlow\",\n",
    "    \"GM-MinFlow\",\n",
    "    \"NGM-MinFlow\",\n",
    "    \"YR-MedianFlow\",\n",
    "    \"GM-MedianFlow\",\n",
    "    \"NGM-MedianFlow\",\n",
    "    \"YR-Q95Flow\",\n",
    "    \"GM-Q95Flow\",\n",
    "    \"NGM-Q95Flow\",\n",
    "    \"YR-Q5Flow\",\n",
    "    \"GM-Q5Flow\",\n",
    "    \"NGM-Q5Flow\",\n",
    "]\n",
    "\n",
    "def functional_pca(drop_metrics: list = drop_metrics):\n",
    "    functional_path = os.path.join(directory, \"computed_data\", \"flow_data\")\n",
    "    functional_files = glob.glob(f\"{functional_path}/*.csv\")\n",
    "\n",
    "    all_top_load_dfs, all_top_metrics = [], {}\n",
    "\n",
    "    for file in functional_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df = df.set_index(df.columns[0])\n",
    "\n",
    "        # drop metrics and checking for missing values.\n",
    "        functional_df = df.loc[:, ~df.columns.str.startswith(tuple(drop_metrics))]\n",
    "        functional_df = functional_df.fillna(functional_df.median())\n",
    "\n",
    "        pca_loadings = pca.loadings(functional_df)\n",
    "\n",
    "        explained_var = pca.explained_variance(functional_df)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_metrics = pca.top_attributes(pca_loadings, 5)\n",
    "        all_top_metrics[file.split(\"/\")[-1].split(\".\")[0]] = top_metrics\n",
    "        new_keys = {'PC1': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC1_{pc1_val}', 'PC2': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC2_{pc2_val}'}\n",
    "        renamed_top_metrics = {new_keys[key]: value for key, value in top_metrics.items()}\n",
    "        yr_top_load_df = pd.DataFrame(renamed_top_metrics)\n",
    "        all_top_load_dfs.append(yr_top_load_df)\n",
    "\n",
    "    # out_put_df = pd.concat(all_top_load_dfs, axis = 1)\n",
    "    # out_put_df =out_put_df.reindex(sorted(out_put_df.columns), axis=1)\n",
    "\n",
    "    return all_top_metrics\n",
    "\n",
    "functionalAttributes = functional_pca()\n",
    "# print(selectTopAtttributesForYear(functionalAttributes))\n",
    "print(functionalAttributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AnnualTotalPrcp_', 'AnnualTotalSnowFall_', 'AnnualMaxSWE_', 'AnnualSumOfRainOnSWE_', 'AnnualNumberRainOnSWEdays_']\n"
     ]
    }
   ],
   "source": [
    "# pca for climate indices\n",
    "def climate_pca():\n",
    "    climate_path = os.path.join(directory, \"computed_data\", \"climate_data\")\n",
    "    climate_files = glob.glob(f\"{climate_path}/*.csv\")\n",
    "\n",
    "    all_top_load_dfs, all_top_metrics = [], {}\n",
    "\n",
    "    for file in climate_files:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "        stations_list = sorted(stations_list)\n",
    "\n",
    "        df.columns.values[0] = \"station_id\"\n",
    "        df['station_id'] = df['station_id'].apply(lambda val: val.lstrip(\"0\") if val.startswith(\"04\") else val)\n",
    "\n",
    "        climate_df = df[df[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        # sort the climate_df\n",
    "        climate_df = climate_df.sort_values(\"station_id\")\n",
    "        climate_df.index = range(len(climate_df))\n",
    "\n",
    "        climate_df = climate_df.drop(columns=[\"station_id\"])\n",
    "\n",
    "        # drop metrics and checking for missing values.\n",
    "        climatedfNomissingValues = climate_df.fillna(climate_df.median())\n",
    "\n",
    "        pca_loadings = pca.loadings(climatedfNomissingValues)\n",
    "\n",
    "        explained_var = pca.explained_variance(climatedfNomissingValues)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_metrics = pca.top_attributes(pca_loadings, 5)\n",
    "        all_top_metrics[file.split(\"/\")[-1].split(\".\")[0]] = top_metrics\n",
    "        new_keys = {'PC1': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC1_{pc1_val}', 'PC2': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC2_{pc2_val}'}\n",
    "        renamed_top_metrics = {new_keys[key]: value for key, value in top_metrics.items()}\n",
    "        yr_top_load_df = pd.DataFrame(renamed_top_metrics)\n",
    "        all_top_load_dfs.append(yr_top_load_df)\n",
    "\n",
    "        out_put_df = pd.concat(all_top_load_dfs, axis = 1)\n",
    "        out_put_df =out_put_df.reindex(sorted(out_put_df.columns), axis=1)\n",
    "\n",
    "    return all_top_metrics\n",
    "\n",
    "climateAttributes = climate_pca()\n",
    "print(selectTopAtttributesForYear(climateAttributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>NGM-RBI-2020</th>\n",
       "      <th>YR-CVQ-2020</th>\n",
       "      <th>GM-CVQ-2020</th>\n",
       "      <th>NGM-CVQ-2020</th>\n",
       "      <th>Specific-YR-MedianFlow-2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02GA003</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.3909</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>1.2703</td>\n",
       "      <td>0.607088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02GA005</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2.9480</td>\n",
       "      <td>1.1331</td>\n",
       "      <td>2.4142</td>\n",
       "      <td>0.564604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02GA010</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.1678</td>\n",
       "      <td>0.8050</td>\n",
       "      <td>1.9489</td>\n",
       "      <td>0.497875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02GA014</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.9411</td>\n",
       "      <td>1.3275</td>\n",
       "      <td>1.6394</td>\n",
       "      <td>0.598141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02GA015</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.2562</td>\n",
       "      <td>0.4745</td>\n",
       "      <td>1.1294</td>\n",
       "      <td>0.514268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>4215500</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.2210</td>\n",
       "      <td>1.5750</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>0.931192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4216418</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.3795</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>1.1497</td>\n",
       "      <td>0.778192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>4217000</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.3004</td>\n",
       "      <td>1.4373</td>\n",
       "      <td>1.0703</td>\n",
       "      <td>0.583968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>4218000</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.0934</td>\n",
       "      <td>1.2758</td>\n",
       "      <td>0.8586</td>\n",
       "      <td>0.668873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>4218518</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.9067</td>\n",
       "      <td>1.1590</td>\n",
       "      <td>0.7471</td>\n",
       "      <td>1.197019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    station_id  NGM-RBI-2020  YR-CVQ-2020  GM-CVQ-2020  NGM-CVQ-2020  \\\n",
       "0      02GA003          0.24       1.3909       0.3529        1.2703   \n",
       "1      02GA005          0.54       2.9480       1.1331        2.4142   \n",
       "2      02GA010          0.39       2.1678       0.8050        1.9489   \n",
       "3      02GA014          0.41       1.9411       1.3275        1.6394   \n",
       "4      02GA015          0.15       1.2562       0.4745        1.1294   \n",
       "..         ...           ...          ...          ...           ...   \n",
       "128    4215500          0.48       1.2210       1.5750        0.9835   \n",
       "129    4216418          0.45       1.3795       1.5005        1.1497   \n",
       "130    4217000          0.37       1.3004       1.4373        1.0703   \n",
       "131    4218000          0.22       1.0934       1.2758        0.8586   \n",
       "132    4218518          0.30       0.9067       1.1590        0.7471   \n",
       "\n",
       "     Specific-YR-MedianFlow-2020  \n",
       "0                       0.607088  \n",
       "1                       0.564604  \n",
       "2                       0.497875  \n",
       "3                       0.598141  \n",
       "4                       0.514268  \n",
       "..                           ...  \n",
       "128                     0.931192  \n",
       "129                     0.778192  \n",
       "130                     0.583968  \n",
       "131                     0.668873  \n",
       "132                     1.197019  \n",
       "\n",
       "[133 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate new dataset for AP classification (riverflows metrics - year and seasonal).\n",
    "def selected_func_metrics() -> dict:\n",
    "\n",
    "    stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "\n",
    "    func_metrics = {}\n",
    "\n",
    "    # riveflow metrics:\n",
    "    functionalAttributes = selectTopAtttributesForYear(functional_pca())\n",
    "    for year in range(2011, 2021):\n",
    "        functionalAttributesWithYear = [functionalAttributes[i] + str(year) for i in range(len(functionalAttributes))]\n",
    "\n",
    "        functional_path = os.path.join(directory, \"computed_data\", \"flow_data\", f'metrics{year}.csv')\n",
    "        functional_df = pd.read_csv(functional_path)\n",
    "        functional_df = functional_df[functionalAttributesWithYear]\n",
    "        functional_df['station_id'] = stations_list\n",
    "        functional_df = functional_df[[\"station_id\"] + [col for col in functional_df.columns if col != \"station_id\"]]\n",
    "        func_metrics[str(year)] = functional_df\n",
    "        dir = os.path.join(output_dir, \"pca_results\",\"func\")\n",
    "        # functional_df.to_csv(dir +  f\"/{year}_func_metrics.csv\", index=False)\n",
    "\n",
    "    return functional_df\n",
    "\n",
    "selected_func_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 414.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>NGM-CVQ-2020</th>\n",
       "      <th>YR-CVQ-2020</th>\n",
       "      <th>GM-CVQ-2020</th>\n",
       "      <th>Specific-YR-MedianFlow-2020</th>\n",
       "      <th>NGM-RBI-2020</th>\n",
       "      <th>AnnualTotalPrcp_2020</th>\n",
       "      <th>AnnualTotalSnowFall_2020</th>\n",
       "      <th>AnnualMaxSWE_2020</th>\n",
       "      <th>AnnualSumOfRainOnSWE_2020</th>\n",
       "      <th>AnnualNumberRainOnSWEdays_2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02GA003</td>\n",
       "      <td>1.2703</td>\n",
       "      <td>1.3909</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.607088</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1010.713959</td>\n",
       "      <td>195.079240</td>\n",
       "      <td>81.669326</td>\n",
       "      <td>203.024210</td>\n",
       "      <td>27.314734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02GA005</td>\n",
       "      <td>2.4142</td>\n",
       "      <td>2.9480</td>\n",
       "      <td>1.1331</td>\n",
       "      <td>0.564604</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1095.710944</td>\n",
       "      <td>220.902053</td>\n",
       "      <td>106.514529</td>\n",
       "      <td>245.755275</td>\n",
       "      <td>30.729560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02GA010</td>\n",
       "      <td>1.9489</td>\n",
       "      <td>2.1678</td>\n",
       "      <td>0.8050</td>\n",
       "      <td>0.497875</td>\n",
       "      <td>0.39</td>\n",
       "      <td>882.545058</td>\n",
       "      <td>151.788813</td>\n",
       "      <td>46.130942</td>\n",
       "      <td>158.554805</td>\n",
       "      <td>20.182011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02GA014</td>\n",
       "      <td>1.6394</td>\n",
       "      <td>1.9411</td>\n",
       "      <td>1.3275</td>\n",
       "      <td>0.598141</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1198.672760</td>\n",
       "      <td>262.683061</td>\n",
       "      <td>131.524397</td>\n",
       "      <td>256.889547</td>\n",
       "      <td>36.047934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02GA015</td>\n",
       "      <td>1.1294</td>\n",
       "      <td>1.2562</td>\n",
       "      <td>0.4745</td>\n",
       "      <td>0.514268</td>\n",
       "      <td>0.15</td>\n",
       "      <td>966.132549</td>\n",
       "      <td>189.796726</td>\n",
       "      <td>81.581732</td>\n",
       "      <td>198.676942</td>\n",
       "      <td>28.311284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>4215500</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>1.2210</td>\n",
       "      <td>1.5750</td>\n",
       "      <td>0.931192</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1246.779627</td>\n",
       "      <td>199.612023</td>\n",
       "      <td>68.894627</td>\n",
       "      <td>196.550519</td>\n",
       "      <td>33.369565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4216418</td>\n",
       "      <td>1.1497</td>\n",
       "      <td>1.3795</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.778192</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1207.922529</td>\n",
       "      <td>221.861610</td>\n",
       "      <td>80.690176</td>\n",
       "      <td>216.901936</td>\n",
       "      <td>41.870588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>4217000</td>\n",
       "      <td>1.0703</td>\n",
       "      <td>1.3004</td>\n",
       "      <td>1.4373</td>\n",
       "      <td>0.583968</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1151.104473</td>\n",
       "      <td>197.834512</td>\n",
       "      <td>64.415240</td>\n",
       "      <td>178.932813</td>\n",
       "      <td>36.728435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>4218000</td>\n",
       "      <td>0.8586</td>\n",
       "      <td>1.0934</td>\n",
       "      <td>1.2758</td>\n",
       "      <td>0.668873</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1117.032122</td>\n",
       "      <td>168.475242</td>\n",
       "      <td>43.875742</td>\n",
       "      <td>134.829480</td>\n",
       "      <td>28.986130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>4218518</td>\n",
       "      <td>0.7471</td>\n",
       "      <td>0.9067</td>\n",
       "      <td>1.1590</td>\n",
       "      <td>1.197019</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1158.703571</td>\n",
       "      <td>149.563787</td>\n",
       "      <td>29.153673</td>\n",
       "      <td>110.838944</td>\n",
       "      <td>22.918367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    station_id  NGM-CVQ-2020  YR-CVQ-2020  GM-CVQ-2020  \\\n",
       "0      02GA003        1.2703       1.3909       0.3529   \n",
       "1      02GA005        2.4142       2.9480       1.1331   \n",
       "2      02GA010        1.9489       2.1678       0.8050   \n",
       "3      02GA014        1.6394       1.9411       1.3275   \n",
       "4      02GA015        1.1294       1.2562       0.4745   \n",
       "..         ...           ...          ...          ...   \n",
       "128    4215500        0.9835       1.2210       1.5750   \n",
       "129    4216418        1.1497       1.3795       1.5005   \n",
       "130    4217000        1.0703       1.3004       1.4373   \n",
       "131    4218000        0.8586       1.0934       1.2758   \n",
       "132    4218518        0.7471       0.9067       1.1590   \n",
       "\n",
       "     Specific-YR-MedianFlow-2020  NGM-RBI-2020  AnnualTotalPrcp_2020  \\\n",
       "0                       0.607088          0.24           1010.713959   \n",
       "1                       0.564604          0.54           1095.710944   \n",
       "2                       0.497875          0.39            882.545058   \n",
       "3                       0.598141          0.41           1198.672760   \n",
       "4                       0.514268          0.15            966.132549   \n",
       "..                           ...           ...                   ...   \n",
       "128                     0.931192          0.48           1246.779627   \n",
       "129                     0.778192          0.45           1207.922529   \n",
       "130                     0.583968          0.37           1151.104473   \n",
       "131                     0.668873          0.22           1117.032122   \n",
       "132                     1.197019          0.30           1158.703571   \n",
       "\n",
       "     AnnualTotalSnowFall_2020  AnnualMaxSWE_2020  AnnualSumOfRainOnSWE_2020  \\\n",
       "0                  195.079240          81.669326                 203.024210   \n",
       "1                  220.902053         106.514529                 245.755275   \n",
       "2                  151.788813          46.130942                 158.554805   \n",
       "3                  262.683061         131.524397                 256.889547   \n",
       "4                  189.796726          81.581732                 198.676942   \n",
       "..                        ...                ...                        ...   \n",
       "128                199.612023          68.894627                 196.550519   \n",
       "129                221.861610          80.690176                 216.901936   \n",
       "130                197.834512          64.415240                 178.932813   \n",
       "131                168.475242          43.875742                 134.829480   \n",
       "132                149.563787          29.153673                 110.838944   \n",
       "\n",
       "     AnnualNumberRainOnSWEdays_2020  \n",
       "0                         27.314734  \n",
       "1                         30.729560  \n",
       "2                         20.182011  \n",
       "3                         36.047934  \n",
       "4                         28.311284  \n",
       "..                              ...  \n",
       "128                       33.369565  \n",
       "129                       41.870588  \n",
       "130                       36.728435  \n",
       "131                       28.986130  \n",
       "132                       22.918367  \n",
       "\n",
       "[133 rows x 11 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate new dataset for AP classification (riverflows metrics - climate indices).\n",
    "def selected_func_climate_attrs() -> pd.DataFrame:\n",
    "    climateAttributes = selectTopAtttributesForYear(climate_pca())\n",
    "\n",
    "    for year in tqdm(range(2011, 2021)):\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(\n",
    "                directory,\n",
    "                \"computed_data\",\n",
    "                \"climate_data\",\n",
    "                f\"climate_indices_{str(year)}.csv\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "\n",
    "        stations_list = sorted(stations_list)\n",
    "\n",
    "        df.columns.values[0] = \"station_id\"\n",
    "        df['station_id'] = df['station_id'].apply(lambda val: val.lstrip(\"0\") if val.startswith(\"04\") else val)\n",
    "\n",
    "        climate_df = df[df[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        # sort the climate_df\n",
    "        climate_df = climate_df.sort_values(\"station_id\")\n",
    "        climate_df.index = range(len(climate_df))\n",
    "\n",
    "        climateAttributesWithYear = [climateAttributes[i] + str(year) for i in range(len(climateAttributes))]\n",
    "        climate_df = climate_df[climateAttributesWithYear]\n",
    "\n",
    "        dir = os.path.join(output_dir, \"pca_results\", \"func\", f\"{year}_func_metrics.csv\")\n",
    "        functional_df = pd.read_csv(dir)\n",
    "\n",
    "        output_df = pd.concat([functional_df, climate_df], axis=1)\n",
    "\n",
    "        output_df = output_df[[\"station_id\"] + [col for col in output_df.columns if col != \"station_id\"]]\n",
    "\n",
    "        # output_df.to_csv(os.path.join(output_dir, \"pca_results\", \"func_climate\", f\"{year}_func_climate_attrs.csv\"), index=False)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "selected_func_climate_attrs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 21.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['station_id', 'mean_topsoil_silt', 'median_topsoil_sand',\n",
       "       'min_topsoil_sand', 'std_topsoil_clay', 'mean_topsoil_sand',\n",
       "       'max_topsoil_sand', 'Undefined', 'End moraine', 'Ground moraine',\n",
       "       'Alluvium', 'Lacustrine', 'Kames and esker', '%forest', '%water',\n",
       "       '%grassland', '%wetland', '%cropland', 'median_elevation',\n",
       "       'mean_elevation', 'min_elevation', 'range_elevation', 'max_elevation',\n",
       "       'max_slope', 'tomatoes', 'oats', 'soyabeans', 'winterwheat',\n",
       "       'springwheat', 'corn', 'NGM-CVQ-2020', 'YR-CVQ-2020', 'GM-CVQ-2020',\n",
       "       'Specific-YR-MedianFlow-2020', 'NGM-RBI-2020', 'AnnualTotalPrcp_2020',\n",
       "       'AnnualMaxSWE_2020', 'AnnualSumOfRainOnSWE_2020',\n",
       "       'AnnualTotalSnowFall_2020', 'AnnualNumberRainOnSWEdays_2020'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate new dataset for AP classification (riverflow metrics + structural attributes + climate indices)\n",
    "stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "\n",
    "stations_list = sorted(stations_list)\n",
    "\n",
    "\n",
    "for year in tqdm(range(2011, 2021)):\n",
    "    df = pd.read_csv(os.path.join(output_dir, \"pca_results\",\"func_climate\", f\"{year}_func_climate_attrs.csv\"))\n",
    "    output_df = pd.concat([generate_struct_crop_df(year), df], axis=1)\n",
    "    output_df['station_id'] = stations_list\n",
    "    output_df = output_df[[\"station_id\"] + [col for col in output_df.columns if col != \"station_id\"]]\n",
    "    # output_df.to_csv(os.path.join(output_dir, \"pca_results\",\"all_attributes\", f\"{year}_func_struct_climate_attrs.csv\"), index=False)\n",
    "output_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA TESTING and AP clustering for Flow metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 52 iterations.\n",
      "Min Label: 0 Max Label: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# metrics to drop.\n",
    "drop_metrics = [\n",
    "    \"Station Name\",\n",
    "    \"Country\",\n",
    "    \"Watershed-Area\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"YR-MaxFlow\",\n",
    "    \"GM-MaxFlow\",\n",
    "    \"NGM-MaxFlow\",\n",
    "    \"YR-MinFlow\",\n",
    "    \"GM-MinFlow\",\n",
    "    \"NGM-MinFlow\",\n",
    "    \"YR-MedianFlow\",\n",
    "    \"GM-MedianFlow\",\n",
    "    \"NGM-MedianFlow\",\n",
    "    \"YR-Q95Flow\",\n",
    "    \"GM-Q95Flow\",\n",
    "    \"NGM-Q95Flow\",\n",
    "    \"YR-Q5Flow\",\n",
    "    \"GM-Q5Flow\",\n",
    "    \"NGM-Q5Flow\",\n",
    "]\n",
    "\n",
    "\n",
    "functional_path = os.path.join(directory, \"computed_data\", \"flow_data\", \"metrics2011.csv\")\n",
    "functional_df = pd.read_csv(functional_path)\n",
    "functional_df = functional_df.set_index(functional_df.columns[0])\n",
    "functional_df = functional_df.loc[:, ~functional_df.columns.str.startswith(tuple(drop_metrics))]\n",
    "functional_df = functional_df.fillna(functional_df.median())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(functional_df)\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(scaled_df)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])\n",
    "\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "# print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "\n",
    "\n",
    "cumulative_variance_ratio = explained_variance_ratio.cumsum()\n",
    "n_components = len(cumulative_variance_ratio[cumulative_variance_ratio <= 0.95])\n",
    "# print(\"Number of Principal Components to retain 95% variance:\", n_components)\n",
    "\n",
    "\n",
    "# loadings\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "loadings_df = pd.DataFrame(loadings, columns = ['PC1', 'PC2'], index = functional_df.columns)\n",
    "\n",
    "pc1 = loadings_df['PC1']\n",
    "\n",
    "\n",
    "# use the value of pc1 to get the top 5 attributes\n",
    "topLoadingAttributes_pc1 = pc1.abs().sort_values(ascending=False).head(5).index\n",
    "topLoadingAttributes_pc2 = loadings_df['PC2'].abs().sort_values(ascending=False).head(2).index\n",
    "\n",
    "# topLoadingAttributes = list(set(topLoadingAttributes_pc1).union(set(topLoadingAttributes_pc2)))\n",
    "topLoadingAttributes = [\n",
    "        \"YR-RBI-2011\",\n",
    "        \"NGM-RBI-2011\",\n",
    "        \"Specific-GM-MedianFlow-2011\",\n",
    "        \"Specific-NGM-MedianFlow-2011\",\n",
    "        \"YR-CVQ-2011\",\n",
    "        \"Specific-YR-MedianFlow-2011\",\n",
    "        \"Specific-GM-Q95-2011\",\n",
    "        ]\n",
    "\n",
    "\n",
    "resultedTopAttributes_df = functional_df[topLoadingAttributes]\n",
    "\n",
    "# AP\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model = AffinityPropagation(damping=0.9, verbose=2)\n",
    "\n",
    "# fit the model\n",
    "resultedTopAttributes_df = resultedTopAttributes_df.fillna(resultedTopAttributes_df.median())\n",
    "model.fit(resultedTopAttributes_df)\n",
    "labels = model.labels_\n",
    "\n",
    "ap_res = {}\n",
    "ap_res[functional_path.split(\"/\")[-1].split(\".\")[0]] = list(labels)\n",
    "result = pd.DataFrame(ap_res).set_index(resultedTopAttributes_df.index)\n",
    "\n",
    "# min and max values of the labels\n",
    "print(\"Min Label:\", min(labels), \"Max Label:\", max(labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
