{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca import PCA_Analysis\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "directory = os.getenv(\"path\")\n",
    "pca = PCA_Analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting top attributes from computed pca\n",
    "def select_top_attr(all_top_attr, attribute) -> list:\n",
    "    selected_attr = []\n",
    "    top_attr = all_top_attr[attribute]\n",
    "    attrs_list = list(top_attr.values())\n",
    "\n",
    "    for i in range(4):\n",
    "        selected_attr.append(attrs_list[0][i])\n",
    "    for i in range(2):\n",
    "        selected_attr.append(attrs_list[1][i])\n",
    "    return list(set(selected_attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structural attributes (Time-invariant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_pca() -> dict:\n",
    "    # struct_path = os.path.join(directory, \"data\",\"raw_datasets\", \"structural_attributes\", \"140_TI_variables\")\n",
    "    struct_path = os.path.join(directory, \"computed_data\", \"TI_variables\")\n",
    "\n",
    "    stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "\n",
    "    struct_files = os.listdir(struct_path)\n",
    "    struct_files = [file for file in struct_files if file.endswith(\".csv\")]\n",
    "    struct_files.remove('leb_surficial.csv')\n",
    "\n",
    "\n",
    "    all_top_load_dfs, all_top_attr = [], {}\n",
    "\n",
    "    for file in struct_files:\n",
    "        attr_type = file.split(\"_\")[1].split(\".\")[0]\n",
    "        input_struct_data = pd.read_csv(os.path.join(struct_path, file))\n",
    "\n",
    "        input_struct_data = input_struct_data[input_struct_data[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "\n",
    "        struct_data = pd.concat([input_struct_data[\"station_id\"], input_struct_data.iloc[:, 4:]], axis = 1)\n",
    "        struct_data = struct_data.set_index(\"station_id\")\n",
    "\n",
    "        loadings = pca.loadings(struct_data)\n",
    "\n",
    "        explained_var = pca.explained_variance(struct_data)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        print(f'explained variance for {attr_type} PC1: {pc1_val} and PC2: {pc2_val} summed up to {pc1_val + pc2_val}')\n",
    "\n",
    "        top_attr = pca.top_attributes(loadings, 5)\n",
    "        all_top_attr[attr_type] = top_attr\n",
    "        new_keys = {'PC1': f'{attr_type}_PC1_{pc1_val}', 'PC2': f'{attr_type}_PC2_{pc2_val}'}\n",
    "        renamed_top_attr = {new_keys[key]: value for key, value in top_attr.items()}\n",
    "        top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "        all_top_load_dfs.append(top_load_df)\n",
    "    # return  pd.concat(all_top_load_dfs, axis = 1)\n",
    "    # out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"TI_top_attributes.csv\"), index=False)\n",
    "    return all_top_attr\n",
    "\n",
    "structural_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new datasets with selected attributes\n",
    "def generate_struct_df() -> pd.DataFrame:\n",
    "    attr_collection = []\n",
    "    struct_pca = structural_pca()\n",
    "    for key in struct_pca.keys():\n",
    "        attributes = select_top_attr(struct_pca, key)\n",
    "        path = os.path.join(\n",
    "            directory,\n",
    "            \"computed_data\",\n",
    "            \"TI_variables\",\n",
    "            f\"leb_{key}.csv\",\n",
    "        )\n",
    "        stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "        # input_struct_data = input_struct_data[input_struct_data[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        att_file = pd.read_csv(path)\n",
    "        _133_stations_df = att_file[att_file[\"station_id\"].isin(stations_list)]\n",
    "        _133_stations_df = _133_stations_df[attributes]\n",
    "        attr_collection.append(_133_stations_df)\n",
    "\n",
    "    return pd.concat(attr_collection, axis=1).reset_index(drop=True)\n",
    "\n",
    "generate_struct_df().columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop inventories (Time-variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_pca() -> dict:\n",
    "    crops_path = os.path.join(directory, \"computed_data\", \"crop_data\")\n",
    "\n",
    "    crop_inventories_files = glob.glob(f\"{crops_path}/*.csv\")\n",
    "\n",
    "    all_crop_top_load_dfs, all_top_attr  = [], {}\n",
    "\n",
    "    for path in crop_inventories_files:\n",
    "        crop_yr_df = pd.read_csv(path)\n",
    "        crop_yr_df = crop_yr_df.set_index(crop_yr_df.columns[0])\n",
    "\n",
    "        # crop pca analysis\n",
    "        # pca_df = pca.pca_analysis(crop_yr_df)\n",
    "        loadings = pca.loadings(crop_yr_df)\n",
    "\n",
    "        explained_var = pca.explained_variance(crop_yr_df)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        top_attr = pca.top_attributes(loadings, 5)\n",
    "        all_top_attr[path.split(\"/\")[-1].split(\".\")[0]] = top_attr\n",
    "        new_keys = {'PC1': f'{path.split(\"/\")[-1].split(\".\")[0]}_PC1_{pc1_val}', 'PC2': f'{path.split(\"/\")[-1].split(\".\")[0]}_PC2_{pc2_val}'}\n",
    "        renamed_top_attr = {new_keys[key]: value for key, value in top_attr.items()}\n",
    "        top_load_df = pd.DataFrame(renamed_top_attr)\n",
    "        all_crop_top_load_dfs.append(top_load_df)\n",
    "\n",
    "    # crop_out_put_df = pd.concat(all_crop_top_load_dfs, axis = 1)\n",
    "    # crop_out_put_df = crop_out_put_df.reindex(sorted(crop_out_put_df.columns), axis=1)\n",
    "    # crop_out_put_df.to_csv(os.path.join(directory, \"pca_results\", \"crop_top_attributes.csv\"), index=False)\n",
    "\n",
    "    return all_top_attr\n",
    "\n",
    "print(crop_pca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining TI (soil, lucl, terrain) and TV (crop inventory) datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to select top attributes for year variant attributes.\n",
    "def selectTopAtttributesForYear(datatype: dict) -> list:\n",
    "    pca_selected_attr = []\n",
    "    for _,val in datatype.items():\n",
    "        yearTopAttributes, topAttributes = [], []\n",
    "        for _,v in val.items():\n",
    "            yearTopAttributes.append(v)\n",
    "\n",
    "        for i in range(4):\n",
    "            topAttributes.append(yearTopAttributes[0][i])\n",
    "        for i in range(2):\n",
    "            topAttributes.append(yearTopAttributes[1][i])\n",
    "\n",
    "        pca_selected_attr.append(list(set(topAttributes)))\n",
    "\n",
    "    # selected_attr = []\n",
    "    counter = {}\n",
    "    for arr_attrs in pca_selected_attr:\n",
    "        for attr in arr_attrs:\n",
    "            attr = attr.split(\"2\")[0]\n",
    "            if attr in counter:\n",
    "                counter[attr] += 1\n",
    "            else:\n",
    "                counter[attr] = 1\n",
    "\n",
    "    count, selected_attr = 0, set()\n",
    "    count_values = sorted(list(counter.values()), reverse=True)\n",
    "    # print(\"counter_values\", count_values)\n",
    "    lookup_counter = min(count_values[:5])\n",
    "\n",
    "    for key, value in counter.items():\n",
    "        if value >= lookup_counter:\n",
    "            count += 1\n",
    "            selected_attr.add(key)\n",
    "\n",
    "    return list(selected_attr)\n",
    "\n",
    "# testing:\n",
    "# crops_data = crop_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate crop + struct attributes combined dataset.\n",
    "def generate_struct_crop_df(year:int) -> pd.DataFrame:\n",
    "    crops_pca = crop_pca()\n",
    "    selectedAttributesCrops = selectTopAtttributesForYear(crops_pca)\n",
    "    selectedAttributesCropsWithYear = [0]*len(selectedAttributesCrops)\n",
    "\n",
    "    for i in range(len(selectedAttributesCrops)):\n",
    "        selectedAttributesCropsWithYear[i] = f\"{selectedAttributesCrops[i]}{year}\"\n",
    "\n",
    "    for key in crops_pca.keys():\n",
    "        if year == int(key.split(\"_\")[-1]):\n",
    "            # attributes = select_top_attr(crops_pca, key)\n",
    "            path = os.path.join(directory, \"computed_data\",\"crop_data\", f\"{key}.csv\")\n",
    "            attr_file = pd.read_csv(path)\n",
    "            attr_file = attr_file[selectedAttributesCropsWithYear]\n",
    "            attr_file.columns = selectedAttributesCrops\n",
    "\n",
    "            output_df = pd.concat([generate_struct_df(), attr_file], axis=1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return output_df\n",
    "\n",
    "generate_struct_crop_df(2015).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature engineer (functional attributes - riverflow metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics to drop.\n",
    "drop_metrics = [\n",
    "    \"Station Name\",\n",
    "    \"Country\",\n",
    "    \"Watershed-Area\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"YR-MaxFlow\",\n",
    "    \"GM-MaxFlow\",\n",
    "    \"NGM-MaxFlow\",\n",
    "    \"YR-MinFlow\",\n",
    "    \"GM-MinFlow\",\n",
    "    \"NGM-MinFlow\",\n",
    "    \"YR-MedianFlow\",\n",
    "    \"GM-MedianFlow\",\n",
    "    \"NGM-MedianFlow\",\n",
    "    \"YR-Q95Flow\",\n",
    "    \"GM-Q95Flow\",\n",
    "    \"NGM-Q95Flow\",\n",
    "    \"YR-Q5Flow\",\n",
    "    \"GM-Q5Flow\",\n",
    "    \"NGM-Q5Flow\",\n",
    "]\n",
    "\n",
    "def functional_pca(drop_metrics: list = drop_metrics):\n",
    "    functional_path = os.path.join(directory, \"computed_data\", \"flow_data\")\n",
    "    functional_files = glob.glob(f\"{functional_path}/*.csv\")\n",
    "\n",
    "    all_top_load_dfs, all_top_metrics = [], {}\n",
    "\n",
    "    for file in functional_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df = df.set_index(df.columns[0])\n",
    "\n",
    "        # drop metrics and checking for missing values.\n",
    "        functional_df = df.loc[:, ~df.columns.str.startswith(tuple(drop_metrics))]\n",
    "        functional_df = functional_df.fillna(functional_df.median())\n",
    "\n",
    "        pca_loadings = pca.loadings(functional_df)\n",
    "\n",
    "        explained_var = pca.explained_variance(functional_df)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        # print(f'variance for flow metrics {pc1_val + pc2_val}')\n",
    "\n",
    "        top_metrics = pca.top_attributes(pca_loadings, 5)\n",
    "        all_top_metrics[file.split(\"/\")[-1].split(\".\")[0]] = top_metrics\n",
    "        new_keys = {'PC1': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC1_{pc1_val}', 'PC2': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC2_{pc2_val}'}\n",
    "        renamed_top_metrics = {new_keys[key]: value for key, value in top_metrics.items()}\n",
    "        yr_top_load_df = pd.DataFrame(renamed_top_metrics)\n",
    "        all_top_load_dfs.append(yr_top_load_df)\n",
    "\n",
    "    # out_put_df = pd.concat(all_top_load_dfs, axis = 1)\n",
    "    # out_put_df =out_put_df.reindex(sorted(out_put_df.columns), axis=1)\n",
    "\n",
    "    return all_top_metrics\n",
    "\n",
    "functionalAttributes = functional_pca()\n",
    "# print(selectTopAtttributesForYear(functionalAttributes))\n",
    "print(functionalAttributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca for climate indices\n",
    "def climate_pca():\n",
    "    climate_path = os.path.join(directory, \"computed_data\", \"climate_data\")\n",
    "    climate_files = glob.glob(f\"{climate_path}/*.csv\")\n",
    "\n",
    "    all_top_load_dfs, all_top_metrics = [], {}\n",
    "\n",
    "    for file in climate_files:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "        stations_list = sorted(stations_list)\n",
    "\n",
    "        df.columns.values[0] = \"station_id\"\n",
    "        df['station_id'] = df['station_id'].apply(lambda val: val.lstrip(\"0\") if val.startswith(\"04\") else val)\n",
    "\n",
    "        climate_df = df[df[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        # sort the climate_df\n",
    "        climate_df = climate_df.sort_values(\"station_id\")\n",
    "        climate_df.index = range(len(climate_df))\n",
    "\n",
    "        climate_df = climate_df.drop(columns=[\"station_id\"])\n",
    "\n",
    "        # drop metrics and checking for missing values.\n",
    "        climatedfNomissingValues = climate_df.fillna(climate_df.median())\n",
    "\n",
    "        pca_loadings = pca.loadings(climatedfNomissingValues)\n",
    "\n",
    "        explained_var = pca.explained_variance(climatedfNomissingValues)\n",
    "        pc1_val = round(explained_var[\"Explained Variance\"].iloc[1],2)\n",
    "        pc2_val = round(explained_var[\"Explained Variance\"].iloc[2],2)\n",
    "\n",
    "        # print(f'variance for climate {pc1_val + pc2_val}')\n",
    "\n",
    "        top_metrics = pca.top_attributes(pca_loadings, 5)\n",
    "        all_top_metrics[file.split(\"/\")[-1].split(\".\")[0]] = top_metrics\n",
    "        new_keys = {'PC1': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC1_{pc1_val}', 'PC2': f'{file.split(\"/\")[-1].split(\".\")[0]}_PC2_{pc2_val}'}\n",
    "        renamed_top_metrics = {new_keys[key]: value for key, value in top_metrics.items()}\n",
    "        yr_top_load_df = pd.DataFrame(renamed_top_metrics)\n",
    "        all_top_load_dfs.append(yr_top_load_df)\n",
    "\n",
    "        out_put_df = pd.concat(all_top_load_dfs, axis = 1)\n",
    "        out_put_df =out_put_df.reindex(sorted(out_put_df.columns), axis=1)\n",
    "\n",
    "    return all_top_metrics\n",
    "\n",
    "climateAttributes = climate_pca()\n",
    "print(selectTopAtttributesForYear(climateAttributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new dataset for AP classification (riverflows metrics - year and seasonal).\n",
    "def selected_func_metrics() -> dict:\n",
    "\n",
    "    stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "\n",
    "    func_metrics = {}\n",
    "\n",
    "    # riveflow metrics:\n",
    "    functionalAttributes = selectTopAtttributesForYear(functional_pca())\n",
    "    for year in range(2011, 2021):\n",
    "        functionalAttributesWithYear = [functionalAttributes[i] + str(year) for i in range(len(functionalAttributes))]\n",
    "\n",
    "        functional_path = os.path.join(directory, \"computed_data\", \"flow_data\", f'metrics{year}.csv')\n",
    "        functional_df = pd.read_csv(functional_path)\n",
    "        functional_df = functional_df[functionalAttributesWithYear]\n",
    "        functional_df['station_id'] = stations_list\n",
    "        functional_df = functional_df[[\"station_id\"] + [col for col in functional_df.columns if col != \"station_id\"]]\n",
    "        func_metrics[str(year)] = functional_df\n",
    "        dir = os.path.join(directory, \"computed_data\", \"pca_results\",\"func\")\n",
    "        # functional_df.to_csv(dir +  f\"/{year}_func_metrics.csv\", index=False)\n",
    "\n",
    "    return functional_df\n",
    "\n",
    "selected_func_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new dataset for AP classification (riverflows metrics - climate indices).\n",
    "def selected_func_climate_attrs() -> pd.DataFrame:\n",
    "    climateAttributes = selectTopAtttributesForYear(climate_pca())\n",
    "\n",
    "    for year in tqdm(range(2011, 2021)):\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(\n",
    "                directory,\n",
    "                \"computed_data\",\n",
    "                \"climate_data\",\n",
    "                f\"climate_indices_{str(year)}.csv\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "\n",
    "        stations_list = sorted(stations_list)\n",
    "\n",
    "        df.columns.values[0] = \"station_id\"\n",
    "        df['station_id'] = df['station_id'].apply(lambda val: val.lstrip(\"0\") if val.startswith(\"04\") else val)\n",
    "\n",
    "        climate_df = df[df[\"station_id\"].isin(stations_list)]\n",
    "\n",
    "        # sort the climate_df\n",
    "        climate_df = climate_df.sort_values(\"station_id\")\n",
    "        climate_df.index = range(len(climate_df))\n",
    "\n",
    "        climateAttributesWithYear = [climateAttributes[i] + str(year) for i in range(len(climateAttributes))]\n",
    "        climate_df = climate_df[climateAttributesWithYear]\n",
    "\n",
    "        dir = os.path.join(directory, \"computed_data\", \"pca_results\", \"func\", f\"{year}_func_metrics.csv\")\n",
    "        functional_df = pd.read_csv(dir)\n",
    "\n",
    "        output_df = pd.concat([functional_df, climate_df], axis=1)\n",
    "\n",
    "        output_df = output_df[[\"station_id\"] + [col for col in output_df.columns if col != \"station_id\"]]\n",
    "\n",
    "        # output_df.to_csv(os.path.join(output_dir, \"pca_results\", \"func_climate\", f\"{year}_func_climate_attrs.csv\"), index=False)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "selected_func_climate_attrs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new dataset for AP classification (riverflow metrics + structural attributes + climate indices)\n",
    "stations_list = pd.read_csv(os.path.join(directory, \"raw_data\", \"stations_list.csv\")).stations.tolist()\n",
    "\n",
    "stations_list = sorted(stations_list)\n",
    "\n",
    "\n",
    "for year in tqdm(range(2011, 2021)):\n",
    "    df = pd.read_csv(os.path.join(directory,\"computed_data\", \"pca_results\",\"func_climate\", f\"{year}_func_climate_attrs.csv\"))\n",
    "    output_df = pd.concat([generate_struct_crop_df(year), df], axis=1)\n",
    "    output_df['station_id'] = stations_list\n",
    "    output_df = output_df[[\"station_id\"] + [col for col in output_df.columns if col != \"station_id\"]]\n",
    "    # output_df.to_csv(os.path.join(directory,\"computed_data\", \"pca_results\",\"all_attributes\", f\"{year}_func_struct_climate_attrs.csv\"), index=False)\n",
    "output_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA TESTING and AP clustering for Flow metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 52 iterations.\n",
      "Min Label: 0 Max Label: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# metrics to drop.\n",
    "drop_metrics = [\n",
    "    \"Station Name\",\n",
    "    \"Country\",\n",
    "    \"Watershed-Area\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"YR-MaxFlow\",\n",
    "    \"GM-MaxFlow\",\n",
    "    \"NGM-MaxFlow\",\n",
    "    \"YR-MinFlow\",\n",
    "    \"GM-MinFlow\",\n",
    "    \"NGM-MinFlow\",\n",
    "    \"YR-MedianFlow\",\n",
    "    \"GM-MedianFlow\",\n",
    "    \"NGM-MedianFlow\",\n",
    "    \"YR-Q95Flow\",\n",
    "    \"GM-Q95Flow\",\n",
    "    \"NGM-Q95Flow\",\n",
    "    \"YR-Q5Flow\",\n",
    "    \"GM-Q5Flow\",\n",
    "    \"NGM-Q5Flow\",\n",
    "]\n",
    "\n",
    "\n",
    "functional_path = os.path.join(directory, \"computed_data\", \"flow_data\", \"metrics2011.csv\")\n",
    "functional_df = pd.read_csv(functional_path)\n",
    "functional_df = functional_df.set_index(functional_df.columns[0])\n",
    "functional_df = functional_df.loc[:, ~functional_df.columns.str.startswith(tuple(drop_metrics))]\n",
    "functional_df = functional_df.fillna(functional_df.median())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(functional_df)\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(scaled_df)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])\n",
    "\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "# print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "\n",
    "\n",
    "cumulative_variance_ratio = explained_variance_ratio.cumsum()\n",
    "n_components = len(cumulative_variance_ratio[cumulative_variance_ratio <= 0.95])\n",
    "# print(\"Number of Principal Components to retain 95% variance:\", n_components)\n",
    "\n",
    "\n",
    "# loadings\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "loadings_df = pd.DataFrame(loadings, columns = ['PC1', 'PC2'], index = functional_df.columns)\n",
    "\n",
    "pc1 = loadings_df['PC1']\n",
    "\n",
    "\n",
    "# use the value of pc1 to get the top 5 attributes\n",
    "topLoadingAttributes_pc1 = pc1.abs().sort_values(ascending=False).head(5).index\n",
    "topLoadingAttributes_pc2 = loadings_df['PC2'].abs().sort_values(ascending=False).head(2).index\n",
    "\n",
    "# topLoadingAttributes = list(set(topLoadingAttributes_pc1).union(set(topLoadingAttributes_pc2)))\n",
    "topLoadingAttributes = [\n",
    "        \"YR-RBI-2011\",\n",
    "        \"NGM-RBI-2011\",\n",
    "        \"Specific-GM-MedianFlow-2011\",\n",
    "        \"Specific-NGM-MedianFlow-2011\",\n",
    "        \"YR-CVQ-2011\",\n",
    "        \"Specific-YR-MedianFlow-2011\",\n",
    "        \"Specific-GM-Q95-2011\",\n",
    "        ]\n",
    "\n",
    "\n",
    "resultedTopAttributes_df = functional_df[topLoadingAttributes]\n",
    "\n",
    "# AP\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model = AffinityPropagation(damping=0.9, verbose=2)\n",
    "\n",
    "# fit the model\n",
    "resultedTopAttributes_df = resultedTopAttributes_df.fillna(resultedTopAttributes_df.median())\n",
    "model.fit(resultedTopAttributes_df)\n",
    "labels = model.labels_\n",
    "\n",
    "ap_res = {}\n",
    "ap_res[functional_path.split(\"/\")[-1].split(\".\")[0]] = list(labels)\n",
    "result = pd.DataFrame(ap_res).set_index(resultedTopAttributes_df.index)\n",
    "\n",
    "# min and max values of the labels\n",
    "print(\"Min Label:\", min(labels), \"Max Label:\", max(labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
